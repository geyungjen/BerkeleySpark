{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f3c1dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/18 16:59:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark K-means example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d0063a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').\\\n",
    "                       options(header='true', \\\n",
    "                       inferschema='true').\\\n",
    "            load(\"data/iris.csv\",header=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f91d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c0c0b",
   "metadata": {},
   "source": [
    "You can also get the Statistical resutls from the data frame (Unfortunately, it only works for numerical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf32486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+------------------+------------------+---------+\n",
      "|summary|      sepal_length|        sepal_width|      petal_length|       petal_width|  species|\n",
      "+-------+------------------+-------------------+------------------+------------------+---------+\n",
      "|  count|               150|                150|               150|               150|      150|\n",
      "|   mean| 5.843333333333335| 3.0540000000000007|3.7586666666666693|1.1986666666666672|     null|\n",
      "| stddev|0.8280661279778637|0.43359431136217375| 1.764420419952262|0.7631607417008414|     null|\n",
      "|    min|               4.3|                2.0|               1.0|               0.1|   setosa|\n",
      "|    max|               7.9|                4.4|               6.9|               2.5|virginica|\n",
      "+-------+------------------+-------------------+------------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01216c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         features|\n",
      "+-----------------+\n",
      "|[5.1,3.5,1.4,0.2]|\n",
      "|[4.9,3.0,1.4,0.2]|\n",
      "|[4.7,3.2,1.3,0.2]|\n",
      "|[4.6,3.1,1.5,0.2]|\n",
      "|[5.0,3.6,1.4,0.2]|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "transformed=df.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label']).select(\"features\")\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d1c85",
   "metadata": {},
   "source": [
    "Deal With Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f963c0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|         features|  indexedFeatures|\n",
      "+-----------------+-----------------+\n",
      "|[5.1,3.5,1.4,0.2]|[5.1,3.5,1.4,0.2]|\n",
      "|[4.9,3.0,1.4,0.2]|[4.9,3.0,1.4,0.2]|\n",
      "|[4.7,3.2,1.3,0.2]|[4.7,3.2,1.3,0.2]|\n",
      "|[4.6,3.1,1.5,0.2]|[4.6,3.1,1.5,0.2]|\n",
      "|[5.0,3.6,1.4,0.2]|[5.0,3.6,1.4,0.2]|\n",
      "+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \\\n",
    "                               outputCol=\"indexedFeatures\",\\\n",
    "                               maxCategories=4).fit(transformed)\n",
    "\n",
    "data = featureIndexer.transform(transformed)\n",
    "data.show(5,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f8c857",
   "metadata": {},
   "source": [
    "Since clustering algorithms including k-means use distance-based measurements to determine the similarity between data points, Itâ€™s strongly recommended to standardize the data to have a mean of zero and a standard deviation of one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070f2e1",
   "metadata": {},
   "source": [
    "### StandardScaler\n",
    "\n",
    "When your dataset has uneven distribution of values, for example, some feature columns have values in millions, other feature columns have values in range between 0 and 1, you'd better to normalize all columns to have similar range of value distribuiton, Spark StandardScaler is one of such tools.\n",
    "transforms a dataset of Vector rows, normalizing each feature to have unit standard deviation and/or zero mean. It takes parameters:\n",
    "\n",
    "withStd: True by default. Scales the data to unit standard deviation.\n",
    "withMean: False by default. Centers the data with mean before scaling. It will build a dense output, so take care \n",
    "when applying to sparse input.\n",
    "\n",
    "StandardScaler is an Estimator which can be fit on a dataset to produce a StandardScalerModel; this amounts to computing summary statistics. The model can then transform a Vector column in a dataset to have unit standard deviation and/or zero mean features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60e6adaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-------------------------------------------------------------------------+\n",
      "|features         |indexedFeatures  |scaledFeatures                                                           |\n",
      "+-----------------+-----------------+-------------------------------------------------------------------------+\n",
      "|[5.1,3.5,1.4,0.2]|[5.1,3.5,1.4,0.2]|[6.158928408838787,8.072061621390857,0.7934616853039358,0.26206798787142]|\n",
      "|[4.9,3.0,1.4,0.2]|[4.9,3.0,1.4,0.2]|[5.9174018045706,6.9189099611921625,0.7934616853039358,0.26206798787142] |\n",
      "|[4.7,3.2,1.3,0.2]|[4.7,3.2,1.3,0.2]|[5.675875200302412,7.38017062527164,0.7367858506393691,0.26206798787142] |\n",
      "+-----------------+-----------------+-------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"indexedFeatures\",outputCol=\"scaledFeatures\",withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(data)\n",
    "#Normalize each feature to have unit standard deviation. \n",
    "scaledData = scalerModel.transform(data) \n",
    "scaledData.show(n=3, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ef5fb9",
   "metadata": {},
   "source": [
    "### clustering algorithm is for unsupervised learning, therefore, no training and testing splits necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92c9340b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "\n",
    "kmeans = KMeans() \\\n",
    "          .setK(3) \\\n",
    "          .setFeaturesCol(\"scaledFeatures\")\\\n",
    "          .setPredictionCol(\"cluster\")\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[kmeans])\n",
    "\n",
    "model = pipeline.fit(scaledData)\n",
    "\n",
    "cluster = model.transform(scaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "753d53fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+\n",
      "|features         |cluster|\n",
      "+-----------------+-------+\n",
      "|[5.1,3.5,1.4,0.2]|1      |\n",
      "|[4.9,3.0,1.4,0.2]|1      |\n",
      "|[4.7,3.2,1.3,0.2]|1      |\n",
      "|[4.6,3.1,1.5,0.2]|1      |\n",
      "|[5.0,3.6,1.4,0.2]|1      |\n",
      "|[5.4,3.9,1.7,0.4]|1      |\n",
      "|[4.6,3.4,1.4,0.3]|1      |\n",
      "|[5.0,3.4,1.5,0.2]|1      |\n",
      "|[4.4,2.9,1.4,0.2]|1      |\n",
      "|[4.9,3.1,1.5,0.1]|1      |\n",
      "|[5.4,3.7,1.5,0.2]|1      |\n",
      "|[4.8,3.4,1.6,0.2]|1      |\n",
      "|[4.8,3.0,1.4,0.1]|1      |\n",
      "|[4.3,3.0,1.1,0.1]|1      |\n",
      "|[5.8,4.0,1.2,0.2]|1      |\n",
      "|[5.7,4.4,1.5,0.4]|1      |\n",
      "|[5.4,3.9,1.3,0.4]|1      |\n",
      "|[5.1,3.5,1.4,0.3]|1      |\n",
      "|[5.7,3.8,1.7,0.3]|1      |\n",
      "|[5.1,3.8,1.5,0.3]|1      |\n",
      "+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster.select(\"features\", \"cluster\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad8c8d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([7.04524479, 6.17347978, 2.50588155, 1.88127377]),\n",
       " array([6.0454109 , 7.88294475, 0.82973422, 0.31972295]),\n",
       " array([8.22013841, 7.19671468, 3.13005178, 2.59685552])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages[0].clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a1941d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
