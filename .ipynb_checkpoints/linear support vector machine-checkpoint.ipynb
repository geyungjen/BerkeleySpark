{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff90cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c5e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is one time on your PC; you may need to run it everytime on colab and databrick notebooks\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d06a4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "167306f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 22:32:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/16 22:32:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/04/16 22:32:42 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=SparkConf())\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a0369",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine with pyspark¶\n",
    "\n",
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f84d3ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-------+---------+-------+-------+-------+----+--------+--------+--------+-----+--------+---+\n",
      "|age|        job|marital|education|default|balance|housing|loan| contact|duration|campaign|pdays|previous|  y|\n",
      "+---+-----------+-------+---------+-------+-------+-------+----+--------+--------+--------+-----+--------+---+\n",
      "| 30| unemployed|married|  primary|     no|   1787|     no|  no|cellular|      79|       1|   -1|       0| no|\n",
      "| 33|   services|married|secondary|     no|   4789|    yes| yes|cellular|     220|       1|  339|       4| no|\n",
      "| 35| management| single| tertiary|     no|   1350|    yes|  no|cellular|     185|       1|  330|       1| no|\n",
      "| 30| management|married| tertiary|     no|   1476|    yes| yes| unknown|     199|       4|   -1|       0| no|\n",
      "| 59|blue-collar|married|secondary|     no|      0|    yes|  no| unknown|     226|       1|   -1|       0| no|\n",
      "+---+-----------+-------+---------+-------+-------+-------+----+--------+--------+--------+-----+--------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data/SparkData/bank.csv', header=True, inferSchema=True, sep=\";\")\n",
    "df.drop('day','month','poutcome').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "075b10f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669cf677",
   "metadata": {},
   "source": [
    "### Deal with categorical data and Convert the data to dense vector¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22cd7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "catcols = ['job','marital','education','default','housing','loan','contact','poutcome']\n",
    "num_cols = ['balance', 'duration','campaign','pdays','previous']\n",
    "labelCol = 'y'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe9796",
   "metadata": {},
   "source": [
    "### Process categorical columns\n",
    "\n",
    "The following code does three things with pipeline:\n",
    "\n",
    "StringIndexer all categorical columns\n",
    "\n",
    "OneHotEncoder all categorical index columns\n",
    "\n",
    "VectorAssembler all feature columns into one vector column\n",
    "\n",
    "Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32e2442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# categorical columns\n",
    "categorical_columns = catcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c00654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)) for c in categorical_columns ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d3761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), \\\n",
    "                           outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) for indexer in indexers ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a598190",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() \\\n",
    "                                       for encoder in encoders] + num_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3391d049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 22:33:00 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+---------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                 |label|\n",
      "+---------------------------------------------------------------------------------------------------------+-----+\n",
      "|(29,[8,11,15,16,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1787.0,79.0,1.0,-1.0])                |no   |\n",
      "|(29,[4,11,13,16,17,19,22,24,25,26,27,28],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,4789.0,220.0,1.0,339.0,4.0])       |no   |\n",
      "|(29,[0,12,14,16,17,18,19,22,24,25,26,27,28],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1350.0,185.0,1.0,330.0,1.0])|no   |\n",
      "|(29,[0,11,14,16,17,20,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1476.0,199.0,4.0,-1.0])               |no   |\n",
      "|(29,[1,11,13,16,17,18,20,21,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,226.0,1.0,-1.0])                  |no   |\n",
      "+---------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "model=pipeline.fit(df)\n",
    "data = model.transform(df)\n",
    "data = data.withColumn('label',col(labelCol))\n",
    "data=data.select('features','label')\n",
    "data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2f5d9f",
   "metadata": {},
   "source": [
    "### We need to deal with label, which is string, yes or no, need to make them numbers\n",
    "\n",
    "Build StringIndexer stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a20bdf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column \n",
    "labelIndexer = StringIndexer(inputCol='label',\n",
    "                             outputCol='indexedLabel').fit(data)\n",
    "data=labelIndexer.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbefde62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+\n",
      "|            features|label|indexedLabel|\n",
      "+--------------------+-----+------------+\n",
      "|(29,[8,11,15,16,1...|   no|         0.0|\n",
      "|(29,[4,11,13,16,1...|   no|         0.0|\n",
      "|(29,[0,12,14,16,1...|   no|         0.0|\n",
      "|(29,[0,11,14,16,1...|   no|         0.0|\n",
      "|(29,[1,11,13,16,1...|   no|         0.0|\n",
      "+--------------------+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "277e9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous. \n",
    "featureIndexer =VectorIndexer(inputCol=\"features\", \\\n",
    "                                  outputCol=\"indexedFeatures\", \\\n",
    "                                  maxCategories=3).fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad1e6e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+--------------------+\n",
      "|            features|label|indexedLabel|     indexedFeatures|\n",
      "+--------------------+-----+------------+--------------------+\n",
      "|(29,[8,11,15,16,1...|   no|         0.0|(29,[8,11,15,16,1...|\n",
      "|(29,[4,11,13,16,1...|   no|         0.0|(29,[4,11,13,16,1...|\n",
      "|(29,[0,12,14,16,1...|   no|         0.0|(29,[0,12,14,16,1...|\n",
      "|(29,[0,11,14,16,1...|   no|         0.0|(29,[0,11,14,16,1...|\n",
      "|(29,[1,11,13,16,1...|   no|         0.0|(29,[1,11,13,16,1...|\n",
      "+--------------------+-----+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=featureIndexer.transform(data)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49107c78",
   "metadata": {},
   "source": [
    "### Split the data to training and test data sets¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb2b48f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------+-----+------------+------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                        |label|indexedLabel|indexedFeatures                                                                                 |\n",
      "+------------------------------------------------------------------------------------------------+-----+------------+------------------------------------------------------------------------------------------------+\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,11.0,104.0,3.0,-1.0]) |no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,11.0,104.0,3.0,-1.0]) |\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,117.0,635.0,1.0,-1.0])|no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,117.0,635.0,1.0,-1.0])|\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,407.0,145.0,2.0,-1.0])|no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,407.0,145.0,2.0,-1.0])|\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,466.0,164.0,1.0,-1.0])|no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,466.0,164.0,1.0,-1.0])|\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,644.0,54.0,2.0,-1.0]) |no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,644.0,54.0,2.0,-1.0]) |\n",
      "+------------------------------------------------------------------------------------------------+-----+------------+------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------------------------------------------------------------------------------------+-----+------------+--------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                          |label|indexedLabel|indexedFeatures                                                                                   |\n",
      "+--------------------------------------------------------------------------------------------------+-----+------------+--------------------------------------------------------------------------------------------------+\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-588.0,81.0,4.0,-1.0])  |no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-588.0,81.0,4.0,-1.0])  |\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-105.0,60.0,2.0,-1.0])  |no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-105.0,60.0,2.0,-1.0])  |\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,238.0,808.0,1.0,-1.0])  |no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,238.0,808.0,1.0,-1.0])  |\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,857.0,238.0,6.0,-1.0])  |no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,857.0,238.0,6.0,-1.0])  |\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1623.0,1081.0,2.0,-1.0])|yes  |1.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1623.0,1081.0,2.0,-1.0])|\n",
      "+--------------------------------------------------------------------------------------------------+-----+------------+--------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (40% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.6, 0.4])\n",
    "trainingData.show(5,False)\n",
    "testData.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd9692a",
   "metadata": {},
   "source": [
    "### Build cross-validation model¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9b2751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "lsvc = LinearSVC(featuresCol=\"indexedFeatures\", labelCol=\"indexedLabel\", maxIter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34dc72cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "from pyspark.ml.feature import IndexToString\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85b5d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[lsvc,labelConverter])\n",
    "# Train model.  This also runs the indexers.\n",
    "lsvcModel = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379edfee",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d2dc29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+--------------------+--------------------+----------+--------------+\n",
      "|            features|label|indexedLabel|     indexedFeatures|       rawPrediction|prediction|predictedLabel|\n",
      "+--------------------+-----+------------+--------------------+--------------------+----------+--------------+\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[1.51371088455641...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[1.51841371999673...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[0.62331574570784...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[1.35275732000305...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|  yes|         1.0|(29,[0,11,13,16,1...|[0.31426261086845...|       0.0|            no|\n",
      "+--------------------+-----+------------+--------------------+--------------------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = lsvcModel.transform(testData)\n",
    "# Select example rows to display. \n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51f0eb5",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "522378cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9058370044052864\n",
      "Test Error = 0.094163\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy = {accuracy}\")\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc39cb83",
   "metadata": {},
   "source": [
    "### Get confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11414571",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=predictions.select(\"prediction\").collect()\n",
    "y_orig=predictions.select(\"indexedLabel\").collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4632ece9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1596   25]\n",
      " [ 146   49]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf898b6",
   "metadata": {},
   "source": [
    "### Tear down machine learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54e31d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop session \n",
    "sc.stop()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313c549c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
