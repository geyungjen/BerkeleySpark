{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "844d9745",
   "metadata": {},
   "source": [
    "# Reference: Python Data Science Handbook\n",
    "\n",
    "\n",
    " https://jakevdp.github.io/PythonDataScienceHandbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf500ce",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f9aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf754a",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "The introduction of basis functions into our linear regression makes the model much more flexible, but it also can very quickly lead to over-fitting. For example, if we choose too many Gaussian basis functions, we end up with results that don't look so good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a2a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class GaussianFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Uniformly-spaced Gaussian Features for 1D input\"\"\"\n",
    "    \n",
    "    def __init__(self, N, width_factor=2.0):\n",
    "        self.N = N\n",
    "        self.width_factor = width_factor\n",
    "    \n",
    "    @staticmethod\n",
    "    def _gauss_basis(x, y, width, axis=None):\n",
    "        arg = (x - y) / width\n",
    "        return np.exp(-0.5 * np.sum(arg ** 2, axis))\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # create N centers spread along the data range\n",
    "        self.centers_ = np.linspace(X.min(), X.max(), self.N)\n",
    "        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return self._gauss_basis(X[:, :, np.newaxis], self.centers_,\n",
    "                                 self.width_, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc2b772",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "The introduction of basis functions into our linear regression makes the model much more flexible, but it also can very quickly lead to over-fitting. For example, if we choose too many Gaussian basis functions, we end up with results that don't look so good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb9a5eef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xfit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-02343d08132e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxfit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxfit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xfit' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = np.sin(x) + 0.1 * rng.randn(50)  \n",
    "\n",
    "model = make_pipeline(GaussianFeatures(30),\n",
    "                      LinearRegression())\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, model.predict(xfit[:, np.newaxis]))\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-1.5, 1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ddab17",
   "metadata": {},
   "source": [
    "With the data projected to the 30-dimensional basis, the model has far too much flexibility and goes to extreme values between locations where it is constrained by data.\n",
    "We can see the reason for this if we plot the coefficients of the Gaussian bases with respect to their locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b04f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basis_plot(model, title=None):\n",
    "    fig, ax = plt.subplots(2, sharex=True)\n",
    "    model.fit(x[:, np.newaxis], y)\n",
    "    ax[0].scatter(x, y)\n",
    "    ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))\n",
    "    ax[0].set(xlabel='x', ylabel='y', ylim=(-1.5, 1.5))\n",
    "    \n",
    "    if title:\n",
    "        ax[0].set_title(title)\n",
    "\n",
    "    ax[1].plot(model.steps[0][1].centers_,\n",
    "               model.steps[1][1].coef_)\n",
    "    ax[1].set(xlabel='basis location',\n",
    "              ylabel='coefficient',\n",
    "              xlim=(0, 10))\n",
    "  \n",
    "model = make_pipeline(GaussianFeatures(30), LinearRegression())\n",
    "basis_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd691dc",
   "metadata": {},
   "source": [
    "The lower panel of this figure shows the amplitude of the basis function at each location. This is typical over-fitting behavior when basis functions overlap: the coefficients of adjacent basis functions blow up and cancel each other out. We know that such behavior is problematic, and it would be nice if we could limit such spikes expliticly in the model by penalizing large values of the model parameters. Such a penalty is known as regularization, and comes in several forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575d5a63",
   "metadata": {},
   "source": [
    "### Ridge regression ($L_2$ Regularization)\n",
    "\n",
    "Perhaps the most common form of regularization is known as *ridge regression* or $L_2$ *regularization*, sometimes also called *Tikhonov regularization*.\n",
    "This proceeds by penalizing the sum of squares (2-norms) of the model coefficients; in this case, the penalty on the model fit would be \n",
    "$$\n",
    "P = \\alpha\\sum_{n=1}^N \\theta_n^2\n",
    "$$\n",
    "where $\\alpha$ is a free parameter that controls the strength of the penalty.\n",
    "This type of penalized model is built into Scikit-Learn with the ``Ridge`` estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e505ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "model = make_pipeline(GaussianFeatures(30), Ridge(alpha=0.1))\n",
    "basis_plot(model, title='Ridge Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e59246",
   "metadata": {},
   "source": [
    "The $\\alpha$ parameter is essentially a knob controlling the complexity of the resulting model.\n",
    "In the limit $\\alpha \\to 0$, we recover the standard linear regression result; in the limit $\\alpha \\to \\infty$, all model responses will be suppressed.\n",
    "One advantage of ridge regression in particular is that it can be computed very efficiently—at hardly more computational cost than the original linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d473c77c",
   "metadata": {},
   "source": [
    "### Lasso regression ($L_1$ regularization)\n",
    "\n",
    "Another very common type of regularization is known as lasso, and involves penalizing the sum of absolute values (1-norms) of regression coefficients:\n",
    "$$\n",
    "P = \\alpha\\sum_{n=1}^N |\\theta_n|\n",
    "$$\n",
    "Though this is conceptually very similar to ridge regression, the results can differ surprisingly: for example, due to geometric reasons lasso regression tends to favor *sparse models* where possible: that is, it preferentially sets model coefficients to exactly zero.\n",
    "\n",
    "We can see this behavior in duplicating the ridge regression figure, but using L1-normalized coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model = make_pipeline(GaussianFeatures(30), Lasso(alpha=0.001))\n",
    "basis_plot(model, title='Lasso Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66aa015",
   "metadata": {},
   "source": [
    "With the lasso regression penalty, the majority of the coefficients are exactly zero, with the functional behavior being modeled by a small subset of the available basis functions. As with ridge regularization, the 𝛼 parameter tunes the strength of the penalty, and should be determined via, for example, cross-validation (refer back to Hyperparameters and Model Validation for a discussion of this).\n",
    "\n",
    "![](figures/ridge_vs_lasso_all.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670bb4c1",
   "metadata": {},
   "source": [
    "## Example: Predicting Bicycle Traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f4b48",
   "metadata": {},
   "source": [
    "As an example, let's take a look at whether we can predict the number of bicycle trips across Seattle's Fremont Bridge based on weather, season, and other factors.\n",
    "\n",
    "In this section, we will join the bike data with another dataset, and try to determine the extent to which weather and seasonal factors—temperature, precipitation, and daylight hours—affect the volume of bicycle traffic through this corridor.\n",
    "Fortunately, the NOAA makes available their daily [weather station data](http://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND) (I used station ID USW00024233) and we can easily use Pandas to join the two data sources.\n",
    "We will perform a simple linear regression to relate weather and other information to bicycle counts, in order to estimate how a change in any one of these parameters affects the number of riders on a given day.\n",
    "\n",
    "In particular, this is an example of how the tools of Scikit-Learn can be used in a statistical modeling framework, in which the parameters of the model are assumed to have interpretable meaning.\n",
    "As discussed previously, this is not a standard approach within machine learning, but such interpretation is possible for some models.\n",
    "\n",
    "Let's start by loading the two datasets, indexing by date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b6b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl -o FremontBridge.csv https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a996c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "counts = pd.read_csv('data/FremontBridge.csv', index_col='Date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55833f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('data/BicycleWeather.csv', index_col='DATE', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05ce411",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d1759",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = counts.resample('d').sum()\n",
    "daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88dd8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily['Total'] = daily.sum(axis=1)\n",
    "daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfaead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = daily[['Total']] # remove other columns\n",
    "daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5136bf55",
   "metadata": {},
   "source": [
    "The patterns of use generally vary from day to day; let's account for this in our data by adding binary columns that indicate the day of the week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37295a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "for i in range(7):\n",
    "    daily.insert(loc=1, column=days[i], value = (daily.index.dayofweek == i).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de411b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a4e84b",
   "metadata": {},
   "source": [
    "\n",
    "Similarly, we might expect riders to behave differently on holidays; let's add an indicator of this as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b144c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays('2012', '2016')\n",
    "holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2ebdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(1, index=holidays, name='holiday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477222e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = daily.join(pd.Series(1, index=holidays, name='holiday'))\n",
    "daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily[daily['holiday'] == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc5b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily['holiday'].fillna(0, inplace=True)\n",
    "daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily[daily['holiday']==1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1554b90",
   "metadata": {},
   "source": [
    "We also might suspect that the hours of daylight would affect how many people ride; let's use the standard astronomical calculation to add this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f1f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def hours_of_daylight(date, axis=23.44, latitude=47.61):\n",
    "    \"\"\"Compute the hours of daylight for the given date\"\"\"\n",
    "    days = (date - datetime.datetime(2000, 12, 21)).days\n",
    "    m = (1. - np.tan(np.radians(latitude))\n",
    "         * np.tan(np.radians(axis) * np.cos(days * 2 * np.pi / 365.25)))\n",
    "    return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180.\n",
    "\n",
    "daily['daylight_hrs'] = list(map(hours_of_daylight, daily.index))\n",
    "daily[['daylight_hrs']].plot()\n",
    "plt.ylim(8, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d64e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ecedeb",
   "metadata": {},
   "source": [
    "We can also add the average temperature and total precipitation to the data. In addition to the inches of precipitation, let's add a flag that indicates whether a day is dry (has zero precipitation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0321f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperatures are in 1/10 deg C; convert to C\n",
    "weather['TMIN'] /= 10\n",
    "weather['TMAX'] /= 10\n",
    "weather['Temp (C)'] = 0.5 * (weather['TMIN'] + weather['TMAX'])\n",
    "\n",
    "# precip is in 1/10 mm; convert to inches\n",
    "weather['PRCP'] /= 254\n",
    "weather['dry day'] = (weather['PRCP'] == 0).astype(int)\n",
    "\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c35d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a225d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = daily.join(weather[['PRCP', 'Temp (C)', 'dry day']])\n",
    "daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a937b8e4",
   "metadata": {},
   "source": [
    "Finally, let's add a counter that increases from day 1, and measures how many years have passed. This will let us measure any observed annual increase or decrease in daily crossings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89999c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily['annual'] = (daily.index - daily.index[0]).days / 365."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b8cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8bbd72",
   "metadata": {},
   "source": [
    "With this in place, we can choose the columns to use, and fit a linear regression model to our data. We will set fit_intercept = False, because the daily flags essentially operate as their own day-specific intercepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68186df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with null values\n",
    "daily.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "column_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'holiday',\n",
    "                'daylight_hrs', 'PRCP', 'dry day', 'Temp (C)', 'annual']\n",
    "X = daily[column_names]\n",
    "y = daily['Total']\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X, y)\n",
    "daily['predicted'] = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57069b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91fc48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily[['Total', 'predicted']].plot(alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db8165a",
   "metadata": {},
   "source": [
    "It is evident that we have missed some key features, especially during the summer time. Either our features are not complete (i.e., people decide whether to ride to work based on more than just these) or there are some nonlinear relationships that we have failed to take into account (e.g., perhaps people ride less at both high and low temperatures). Nevertheless, our rough approximation is enough to give us some insights, and we can take a look at the coefficients of the linear model to estimate how much each feature contributes to the daily bicycle count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e42d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pd.Series(model.coef_, index=X.columns)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673829f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "np.random.seed(1)\n",
    "err = np.std([model.fit(*resample(X, y)).coef_ for i in range(1000)], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e96bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame({'effect': params.round(), 'error': err.round()}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd377581",
   "metadata": {},
   "source": [
    "We first see that there is a relatively stable trend in the weekly baseline: there are many more riders on weekdays than on weekends and holidays.\n",
    "We see that for each additional hour of daylight, 129 ± 9 more people choose to ride; a temperature increase of one degree Celsius encourages 65 ± 4 people to grab their bicycle; a dry day means an average of 548 ± 33 more riders, and each inch of precipitation means 665 ± 62 more people leave their bike at home.\n",
    "Once all these effects are accounted for, we see a modest increase of 27 ± 18 new daily riders each year.\n",
    "\n",
    "Our model is almost certainly missing some relevant information. For example, nonlinear effects (such as effects of precipitation *and* cold temperature) and nonlinear trends within each variable (such as disinclination to ride at very cold and very hot temperatures) cannot be accounted for in this model.\n",
    "Additionally, we have thrown away some of the finer-grained information (such as the difference between a rainy morning and a rainy afternoon), and we have ignored correlations between days (such as the possible effect of a rainy Tuesday on Wednesday's numbers, or the effect of an unexpected sunny day after a streak of rainy days)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cae9a0",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddff861",
   "metadata": {},
   "source": [
    "Logistic regression is a statistical method for predicting binary classes. The outcome or target variable is dichotomous in nature. Dichotomous means there are only two possible classes. For example, it can be used for cancer detection problems. It computes the probability of an event occurrence.\n",
    "\n",
    "It is a special case of linear regression where the target variable is categorical in nature. Logistic Regression predicts the probability of occurrence of a binary event utilizing a sigmoid function.\n",
    "\n",
    "![](figures/linear_vs_logistic_regression.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777ea91a",
   "metadata": {},
   "source": [
    "### The Logistic Function\n",
    "\n",
    "\n",
    "$$ f(t) = \\frac{e^t}{1 + e^t} = \\frac{1}{1 + e^{-t}} $$\n",
    "\n",
    "![](figures/sigmoid.png)\n",
    "\n",
    "\n",
    "This is the logistic function, also known as the sigmoid function. Note that as $t$ approaches infinity, the value of the logistic function approaches 1 and as t approaches negative infinity, the value of the logistic function approaches 0.\n",
    "\n",
    "We will use $ t = \\sum_{i=0} \\beta_i x_i $, which means we'll be dealing with a familiar looking linear function.\n",
    "\n",
    "This gives us:\n",
    "\n",
    "$$ p(x) = \\frac{e^{\\sum_{i=0} \\beta_i x_i}}{1 + e^{\\sum_{i=0} \\beta_i x^i}} = \\frac{1}{1 + e^{-\\sum_{i=0} \\beta_i x_i}} $$\n",
    "\n",
    "* p(x) is our hypothesis and it represents the probability of a score of x leading to a win. \n",
    "\n",
    "\\begin{align*}\n",
    "& P(Y = 1 | X) = p(X) \\\\\n",
    "& P(Y = 0 | X) = 1 - p(X)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* $\\beta_i$'s are parameters that we will optimize to best fit our data.\n",
    "\n",
    "### Parameter Estimation\n",
    "\n",
    "We choose $\\bf{\\beta}$ by maxmizing the likelihood of the data.\n",
    "\n",
    "What is the likelihood of seeing a particular data point $(x_i, y_i)$?\n",
    "\n",
    "$$L(\\beta) = P(y_i | x_i) = p(x_i)^{y_i} \\times (1 - p(x_i))^{1 - y_i}$$\n",
    "\n",
    "The likelihood of all data points:\n",
    "\n",
    "$$L(\\beta) = \\prod_{i = 1}^{n} p(x_i)^{y_i} \\times (1 - p(x_i))^{1 - y_i}$$\n",
    "\n",
    "We maximize the **log-likelihood**, which will make the computations easier. \n",
    "\n",
    "\\begin{align*}\n",
    "l(\\beta) & = log(L(\\beta)) \\\\\n",
    "         & = \\sum_{i = 1}^{n} y_i log(p(x_i)) + (1 - y_i) log(1 - p(x_i)) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "### Cost function\n",
    "\n",
    "Maximizing the likelihood or the log-likelihood is equivalent to minimizing the negative log-likelihood of the data. The logistic regression cost function is defined as the average cost,\n",
    "$$ \n",
    "J(\\beta) = - \\frac{1}{n} \\left( \\sum_{i = 1}^{n} y_i log(p(x_i)) + (1 - y_i) log(1 - p(x_i)) \\right) \n",
    "$$\n",
    "\n",
    "### Model Interpretation\n",
    "\n",
    "We define the odds of Y = 1 as,\n",
    "$$\n",
    "\\text{odds} = \\frac{P(Y = 1)}{1 - P(Y = 1)} = e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}\n",
    "$$\n",
    "\n",
    "We interpret the $\\beta's$ in multiplicative terms with respect to the odds. For example, the interpretation of $\\beta_1$ is, holding all the other variables/features fixed, for every increase of 1 in $X_1$, the odds of $Y = 1$ increases by a factor of $e^{\\beta_1}$. \n",
    "\n",
    "\n",
    "### Regularization in Logistic Regression\n",
    "\n",
    "Logistic regression is immensely powerful in many cases but in particular tends to over-fit when there are many features. Weights tend to become skewed in the OLS optimum, partially due to the effect of the logit function on the gradient. Just as in linear regression, the effect can be mitigated using regularization, penalizing the model for using additional predictors.\n",
    "\n",
    "Recall that the overall cost of the data is \n",
    "$$\n",
    "\\text{Total cost} = - \\left( \\sum_{i = 1}^{n} y_i log(p(x_i)) + (1 - y_i) log(1 - p(x_i)) \\right) \n",
    "$$\n",
    "\n",
    "**Lasso Regularization** ($L_{1}$) Effective in eliminating redundant predictors. \n",
    "$$\n",
    "\\text{Total cost} = - \\left( \\sum_{i = 1}^{n} y_i log(p(x_i)) + (1 - y_i) log(1 - p(x_i)) \\right) + \\lambda \\sum_{j = 1}^{p} |\\beta_j|\n",
    "$$\n",
    "\n",
    "**Ridge Regularization** ($L_{2}$) Standard shrinkage when you want to keep predictors and believe that there is collinearity present.\n",
    "$$\n",
    "\\text{Total cost} = - \\left( \\sum_{i = 1}^{n} y_i log(p(x_i)) + (1 - y_i) log(1 - p(x_i)) \\right) + \\lambda \\sum_{j = 1}^{p} \\beta_j^2\n",
    "$$\n",
    "\n",
    "Note:\n",
    "* $n$ is the number of data points\n",
    "* $p$ is the number of features\n",
    "* $\\beta_0$ (the intercept term) is not penalized\n",
    "\n",
    "### Discriminative Model\n",
    "\n",
    "* Attempts to model the conditional probablility, $P(y | x)$ \n",
    "* Example: Logistic Regression, Decision trees, etc\n",
    "* Provides a model for the target variable(s) conditional on the observed variables\n",
    "* Inherently supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c5e48e",
   "metadata": {},
   "source": [
    "### Pima Indian Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas\n",
    "import pandas as pd\n",
    "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "# load dataset\n",
    "pima = pd.read_csv(\"data/diabetes.csv\")\n",
    "pima.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea2f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "pima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562236f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset in features and target variable\n",
    "feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\n",
    "X = pima[feature_cols] # Features\n",
    "y = pima.label # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                        test_size=0.25,\n",
    "                                        random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe1f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1846a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50bf030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=[0,1] # name  of classes\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29bde12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88df30e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9ade08",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "Because of its efficient and straightforward nature, doesn't require high computation power, easy to implement, easily interpretable, used widely by data analyst and scientist. Also, it doesn't require scaling of features. Logistic regression provides a probability score for observations.\n",
    "\n",
    "### Disadvantages\n",
    "Logistic regression is not able to handle a large number of categorical features/variables. It is vulnerable to overfitting. Also, can't solve the non-linear problem with the logistic regression that is why it requires a transformation of non-linear features. Logistic regression will not perform well with independent variables that are not correlated to the target variable and are very similar or correlated to each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d36b36c",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c28351",
   "metadata": {},
   "source": [
    "### What is Normalization?\n",
    "\n",
    "Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n",
    "\n",
    "Here’s the formula for normalization:\n",
    "$$\n",
    "X^\\prime=\\frac{X-X_{min}}{X_{max}-X_{min}}\n",
    "$$\n",
    "\n",
    "Here, $X_{max}$ and $X_{min}$ are the maximum and the minimum values of the feature respectively.\n",
    "\n",
    "* When the value of $X$ is the minimum value in the column, the numerator will be 0, and hence $X^\\prime$ is 0\n",
    "* On the other hand, when the value of $X$ is the maximum value in the column, the numerator is equal to the denominator and thus the value of $X^\\prime$ is 1\n",
    "* If the value of X is between the minimum and the maximum value, then the value of $X^\\prime$ is between 0 and 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b140c41",
   "metadata": {},
   "source": [
    "### What is Standardization?\n",
    "\n",
    "Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\n",
    "\n",
    "Here’s the formula for standardization:\n",
    "$$\n",
    "X^\\prime=\\frac{X-\\mu}{\\sigma}\n",
    "$$\n",
    "where $\\mu$ is the mean of the feature values and Feature scaling: $\\sigma$ is the standard deviation of the feature values. Note that in this case, the values are not restricted to a particular range.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
