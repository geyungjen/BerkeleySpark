{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/15 12:01:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# create entry points to spark\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "sc=SparkContext()\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate functions\n",
    "Two aggregate functions:\n",
    "\n",
    "* `aggregate()`\n",
    "* `aggregateByKey()`\n",
    "\n",
    "### `aggregate(zeroValue, seqOp, combOp)`\n",
    "\n",
    "* **zeroValue** is like a data container. Its structure should match with the data structure of the returned values from the seqOp function.\n",
    "* **seqOp** is a function that takes two arguments: the first argument is the zeroValue and the second argument is an element from the RDD. The zeroValue gets updated with the returned value after every run.\n",
    "* **combOp** is a function that takes two arguments: the first argument is the final zeroValue from one partition and the other is another final zeroValue from another partition.\n",
    "\n",
    "The code below calculates the total sum of squares for **mpg** and **disp** in data set **mtcars**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: get some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(mpg=21.0, disp=160.0),\n",
       " Row(mpg=21.0, disp=160.0),\n",
       " Row(mpg=22.8, disp=108.0),\n",
       " Row(mpg=21.4, disp=258.0),\n",
       " Row(mpg=18.7, disp=360.0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcars_df = spark.read.csv('data/SparkData/mtcars.csv', inferSchema=True, header=True).select(['mpg', 'disp'])\n",
    "mtcars_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: calculate averages of mgp and disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpg mean =  20.090625000000003 ; disp mean =  230.721875\n"
     ]
    }
   ],
   "source": [
    "mpg_mean = mtcars_df.select('mpg').rdd.map(lambda x: x[0]).mean()\n",
    "disp_mean = mtcars_df.select('disp').rdd.map(lambda x: x[0]).mean()\n",
    "print('mpg mean = ', mpg_mean, '; ' \n",
    "      'disp mean = ', disp_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: build **zeroValue, seqOp** and **combOp**\n",
    "\n",
    "We are calculating two TSS. We create a tuple to store two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroValue = (0, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **z** below refers to `zeroValue`. Its values get updated after every run. The **x** refers to an element in an RDD partition. In this case, both **z** and **x** have two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqOp = lambda z, x: (z[0] + (x[0] - mpg_mean)**2, z[1] + (x[1] - disp_mean)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `combOp` function simply aggrate all `zeroValues` into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combOp = lambda px, py: ( px[0] + py[0], px[1] + py[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement `aggregate()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1126.0471874999998, 476184.7946875)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcars_df.rdd.aggregate(zeroValue, seqOp, combOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `aggregateByKey(zeroValue, seqOp, combOp)`\n",
    "\n",
    "This function does similar things as `aggregate()`. The `aggregate()` aggregate all results to the very end, but aggregateByKey() merge results by key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=['hello', 'world', 'good', 'hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_value = 0\n",
    "seqOp = (lambda x, y: x + y)\n",
    "combOp = (lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz=sc.parallelize(x).map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 1), ('world', 1), ('good', 1), ('hello', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 1), ('hello', 2), ('world', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz.aggregateByKey(zero_value,seqOp,combOp).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('data/SparkData/mtcars.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/15 10:37:06 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n",
      " Schema: _c0, mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/user/BerkeleySpark/data/SparkData/mtcars.csv\n",
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|              _c0| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n",
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|        Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|\n",
      "|    Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|\n",
      "|       Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n",
      "|   Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|\n",
      "|Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|\n",
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df=df.select(['cyl', 'mpg', 'disp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "|cyl| mpg| disp|\n",
      "+---+----+-----+\n",
      "|  6|21.0|160.0|\n",
      "|  6|21.0|160.0|\n",
      "|  4|22.8|108.0|\n",
      "|  6|21.4|258.0|\n",
      "|  8|18.7|360.0|\n",
      "+---+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "car_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df.createOrReplaceTempView('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_mean_df=spark.sql(\"select cyl as cylinder, avg(mpg) as mean_mpg, avg(disp) as mean_disp from car group by 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+------------------+\n",
      "|cylinder|          mean_mpg|         mean_disp|\n",
      "+--------+------------------+------------------+\n",
      "|       6| 19.74285714285714|183.31428571428572|\n",
      "|       4|26.663636363636364|105.13636363636364|\n",
      "|       8|15.100000000000003|353.09999999999997|\n",
      "+--------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "car_mean_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+\n",
      "|cylinder| mpg| disp|\n",
      "+--------+----+-----+\n",
      "|       6|21.0|160.0|\n",
      "|       6|21.0|160.0|\n",
      "|       4|22.8|108.0|\n",
      "|       6|21.4|258.0|\n",
      "|       8|18.7|360.0|\n",
      "+--------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+------------------+------------------+\n",
      "|cylinder|          mean_mpg|         mean_disp|\n",
      "+--------+------------------+------------------+\n",
      "|       6| 19.74285714285714|183.31428571428572|\n",
      "|       4|26.663636363636364|105.13636363636364|\n",
      "|       8|15.100000000000003|353.09999999999997|\n",
      "+--------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "car_df=car_df.withColumnRenamed(\"cyl\",\"cylinder\")\n",
    "car_df.show(5)\n",
    "car_mean_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+------------------+-----+------------------+\n",
      "|cylinder| mpg|          mean_mpg| disp|         mean_disp|\n",
      "+--------+----+------------------+-----+------------------+\n",
      "|       6|21.0| 19.74285714285714|160.0|183.31428571428572|\n",
      "|       6|21.0| 19.74285714285714|160.0|183.31428571428572|\n",
      "|       4|22.8|26.663636363636364|108.0|105.13636363636364|\n",
      "|       6|21.4| 19.74285714285714|258.0|183.31428571428572|\n",
      "|       8|18.7|15.100000000000003|360.0|353.09999999999997|\n",
      "+--------+----+------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "car_df.createOrReplaceTempView(\"car_df\")\n",
    "car_mean_df.createOrReplaceTempView(\"car_mean_df\")\n",
    "car_mean_combined_df=spark.sql(\"select a.cylinder, a.mpg, b.mean_mpg, a.disp, b.mean_disp from car_df a inner join \\\n",
    "           car_mean_df b on a.cylinder = b.cylinder\")\n",
    "car_mean_combined_df.show(5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_mean_combined_rdd=car_mean_combined_df.rdd.map(lambda x: (x[0], (x[1], x[2], x[3], x[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, (21.0, 19.74285714285714, 160.0, 183.31428571428572)),\n",
       " (6, (21.0, 19.74285714285714, 160.0, 183.31428571428572)),\n",
       " (4, (22.8, 26.663636363636364, 108.0, 105.13636363636364)),\n",
       " (6, (21.4, 19.74285714285714, 258.0, 183.31428571428572))]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_mean_combined_rdd.take(4)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, (1.5804081632653129, 543.5559183673471)],\n",
       " [6, (1.5804081632653129, 543.5559183673471)],\n",
       " [4, (14.92768595041322, 8.200413223140474)],\n",
       " [6, (2.746122448979596, 5577.955918367346)],\n",
       " [8, (12.959999999999972, 47.61000000000047)]]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_mean_square_rdd=car_mean_combined_rdd.map(lambda x: [x[0], ((x[1][0]-x[1][1])**2, (x[1][2]-x[1][3])**2)])\n",
    "car_mean_square_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the squqre mean error between sum((mpg-mean_mpg)^2) and sum((disp-mean_disp)^) based on same key\n",
    "which is model, and use aggregateByKey API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_value = (0, 0) #zero_value[0] is for mpg, zero_value[1] is for disp\n",
    "seqOp = (lambda x, y: (x[0]+y[0], x[1]+y[1])) \n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, (12.677142857142847, 10364.628571428573)),\n",
       " (4, (203.38545454545448, 7220.825454545454)),\n",
       " (8, (85.19999999999999, 59708.38))]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_mean_square_rdd.aggregateByKey(zero_value, seqOp, combOp).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
