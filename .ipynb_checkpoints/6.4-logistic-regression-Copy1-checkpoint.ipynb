{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 17:46:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/16 17:46:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=SparkConf())\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-------+---------+-------+-------+-------+----+--------+--------+--------+-----+--------+---+\n",
      "|age|        job|marital|education|default|balance|housing|loan| contact|duration|campaign|pdays|previous|  y|\n",
      "+---+-----------+-------+---------+-------+-------+-------+----+--------+--------+--------+-----+--------+---+\n",
      "| 30| unemployed|married|  primary|     no|   1787|     no|  no|cellular|      79|       1|   -1|       0| no|\n",
      "| 33|   services|married|secondary|     no|   4789|    yes| yes|cellular|     220|       1|  339|       4| no|\n",
      "| 35| management| single| tertiary|     no|   1350|    yes|  no|cellular|     185|       1|  330|       1| no|\n",
      "| 30| management|married| tertiary|     no|   1476|    yes| yes| unknown|     199|       4|   -1|       0| no|\n",
      "| 59|blue-collar|married|secondary|     no|      0|    yes|  no| unknown|     226|       1|   -1|       0| no|\n",
      "+---+-----------+-------+---------+-------+-------+-------+----+--------+--------+--------+-----+--------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data/SparkData/bank.csv', header=True, inferSchema=True, sep=\";\")\n",
    "df.drop('day','month','poutcome').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Deal with categorical data and Convert the data to dense vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "catcols = ['job','marital','education','default','housing','loan','contact','poutcome']\n",
    "num_cols = ['balance', 'duration','campaign','pdays','previous']\n",
    "labelCol = 'y'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code does three things with pipeline:\n",
    "\n",
    "* **`StringIndexer`** all categorical columns\n",
    "* **`OneHotEncoder`** all categorical index columns\n",
    "* **`VectorAssembler`** all feature columns into one vector column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# categorical columns\n",
    "categorical_columns = catcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)) for c in categorical_columns ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), \\\n",
    "                           outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) for indexer in indexers ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() \\\n",
    "                                       for encoder in encoders] + num_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 17:46:32 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+---------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                 |label|\n",
      "+---------------------------------------------------------------------------------------------------------+-----+\n",
      "|(29,[8,11,15,16,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1787.0,79.0,1.0,-1.0])                |no   |\n",
      "|(29,[4,11,13,16,17,19,22,24,25,26,27,28],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,4789.0,220.0,1.0,339.0,4.0])       |no   |\n",
      "|(29,[0,12,14,16,17,18,19,22,24,25,26,27,28],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1350.0,185.0,1.0,330.0,1.0])|no   |\n",
      "|(29,[0,11,14,16,17,20,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1476.0,199.0,4.0,-1.0])               |no   |\n",
      "|(29,[1,11,13,16,17,18,20,21,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,226.0,1.0,-1.0])                  |no   |\n",
      "+---------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "model=pipeline.fit(df)\n",
    "data = model.transform(df)\n",
    "data = data.withColumn('label',col(labelCol))\n",
    "data=data.select('features','label')\n",
    "data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to deal with label, which is string, yes or no, need to make them numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build StringIndexer stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column \n",
    "labelIndexer = StringIndexer(inputCol='label',\n",
    "                             outputCol='indexedLabel').fit(data)\n",
    "data=labelIndexer.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+\n",
      "|            features|label|indexedLabel|\n",
      "+--------------------+-----+------------+\n",
      "|(29,[8,11,15,16,1...|   no|         0.0|\n",
      "|(29,[4,11,13,16,1...|   no|         0.0|\n",
      "|(29,[0,12,14,16,1...|   no|         0.0|\n",
      "|(29,[0,11,14,16,1...|   no|         0.0|\n",
      "|(29,[1,11,13,16,1...|   no|         0.0|\n",
      "+--------------------+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous. \n",
    "featureIndexer =VectorIndexer(inputCol=\"features\", \\\n",
    "                                  outputCol=\"indexedFeatures\", \\\n",
    "                                  maxCategories=3).fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+--------------------+\n",
      "|            features|label|indexedLabel|     indexedFeatures|\n",
      "+--------------------+-----+------------+--------------------+\n",
      "|(29,[8,11,15,16,1...|   no|         0.0|(29,[8,11,15,16,1...|\n",
      "|(29,[4,11,13,16,1...|   no|         0.0|(29,[4,11,13,16,1...|\n",
      "|(29,[0,12,14,16,1...|   no|         0.0|(29,[0,12,14,16,1...|\n",
      "|(29,[0,11,14,16,1...|   no|         0.0|(29,[0,11,14,16,1...|\n",
      "|(29,[1,11,13,16,1...|   no|         0.0|(29,[1,11,13,16,1...|\n",
      "+--------------------+-----+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=featureIndexer.transform(data)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data to training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------+-----+------------+------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                        |label|indexedLabel|indexedFeatures                                                                                 |\n",
      "+------------------------------------------------------------------------------------------------+-----+------------+------------------------------------------------------------------------------------------------+\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-588.0,81.0,4.0,-1.0])|no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-588.0,81.0,4.0,-1.0])|\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,11.0,104.0,3.0,-1.0]) |no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,11.0,104.0,3.0,-1.0]) |\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,466.0,164.0,1.0,-1.0])|no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,466.0,164.0,1.0,-1.0])|\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,644.0,54.0,2.0,-1.0]) |no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,644.0,54.0,2.0,-1.0]) |\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,725.0,266.0,1.0,-1.0])|no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,725.0,266.0,1.0,-1.0])|\n",
      "+------------------------------------------------------------------------------------------------+-----+------------+------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------------------------------------------------------------------------------------------+-----+------------+------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                        |label|indexedLabel|indexedFeatures                                                                                 |\n",
      "+------------------------------------------------------------------------------------------------+-----+------------+------------------------------------------------------------------------------------------------+\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-105.0,60.0,2.0,-1.0])|no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-105.0,60.0,2.0,-1.0])|\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,117.0,635.0,1.0,-1.0])|no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,117.0,635.0,1.0,-1.0])|\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,238.0,808.0,1.0,-1.0])|no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,238.0,808.0,1.0,-1.0])|\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,407.0,145.0,2.0,-1.0])|no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,407.0,145.0,2.0,-1.0])|\n",
      "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,857.0,238.0,6.0,-1.0])|no   |0.0         |(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,857.0,238.0,6.0,-1.0])|\n",
      "+------------------------------------------------------------------------------------------------+-----+------------+------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (40% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.6, 0.4])\n",
    "trainingData.show(5,False)\n",
    "testData.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build cross-validation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "logr = LogisticRegression(featuresCol='indexedFeatures', labelCol='indexedLabel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "from pyspark.ml.feature import IndexToString\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[logr,labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------+--------------------+\n",
      "|            features|label|predictedLabel|       rawPrediction|\n",
      "+--------------------+-----+--------------+--------------------+\n",
      "|(29,[0,11,13,16,1...|   no|            no|[3.75959527779490...|\n",
      "|(29,[0,11,13,16,1...|   no|            no|[1.25399762341547...|\n",
      "|(29,[0,11,13,16,1...|   no|            no|[0.52518271647105...|\n",
      "|(29,[0,11,13,16,1...|   no|            no|[3.41260006052906...|\n",
      "|(29,[0,11,13,16,1...|   no|            no|[3.34552406711526...|\n",
      "+--------------------+-----+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "# Select example rows to display. \n",
    "predictions.select(\"features\",\"label\",\"predictedLabel\", \"rawPrediction\").show(5)\n",
    "#predictions.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8932203389830509\n",
      "Test Error = 0.10678\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy = {accuracy}\")\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate training model\n",
    "\n",
    "area under ROC  https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
    "accuracy\n",
    "False positive rate by label\n",
    "\n",
    "True positive rate by label\n",
    "\n",
    "Precision by label\n",
    "\n",
    "Recall by label\n",
    "\n",
    "F-measure by label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = model.stages[0]\n",
    "trainingSummary = lrModel.summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 FPR|                 TPR|\n",
      "+--------------------+--------------------+\n",
      "|                 0.0|                 0.0|\n",
      "|                 0.0|0.006756756756756757|\n",
      "|4.073319755600815E-4|0.010135135135135136|\n",
      "| 8.14663951120163E-4|0.013513513513513514|\n",
      "|0.001629327902240326|0.013513513513513514|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "areaUnderROC: 0.882377387570874\n",
      "accuracy: 0.9098509632860778\n"
     ]
    }
   ],
   "source": [
    "# Obtain the objective per iteration\n",
    "# objectiveHistory = trainingSummary.objectiveHistory\n",
    "# print(\"objectiveHistory:\")\n",
    "# for objective in objectiveHistory:\n",
    "#     print(objective)\n",
    "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "trainingSummary.roc.show(5)\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n",
    "print(f\"accuracy: {str(trainingSummary.accuracy)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+--------------------+--------------------+--------------------+----------+--------------+\n",
      "|            features|label|indexedLabel|     indexedFeatures|       rawPrediction|         probability|prediction|predictedLabel|\n",
      "+--------------------+-----+------------+--------------------+--------------------+--------------------+----------+--------------+\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[3.75959527779490...|[0.97723705531420...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[1.25399762341547...|[0.77799110173584...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[0.52518271647105...|[0.62835885797783...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[3.41260006052906...|[0.96809600612767...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[3.34552406711526...|[0.96595795950420...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[3.02546245404384...|[0.95371126997046...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[3.66147484748907...|[0.97494908383201...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|  yes|         1.0|(29,[0,11,13,16,1...|[-0.5168237077473...|[0.37359525649958...|       1.0|           yes|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[2.39079567224896...|[0.91612272929635...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[4.47174231138494...|[0.98870172139151...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[3.76949810084154...|[0.97745630360547...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|  yes|         1.0|(29,[0,11,13,16,1...|[3.18146001245342...|[0.96013059264831...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[4.10716177909369...|[0.98381195515596...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[2.82902005425898...|[0.94422401566594...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[3.55622977239825...|[0.97224602394770...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[4.62984379044994...|[0.99033798237052...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[4.74011657048551...|[0.99133805725319...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[4.37104837232804...|[0.98751974100982...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[4.97444680295038...|[0.99313511069206...|       0.0|            no|\n",
      "|(29,[0,11,13,16,1...|   no|         0.0|(29,[0,11,13,16,1...|[3.67758984306858...|[0.97533966838598...|       0.0|            no|\n",
      "+--------------------+-----+------------+--------------------+--------------------+--------------------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate by label:\n",
      "label 0:0.6824324324324325\n",
      "label 1:0.018737270875763747\n"
     ]
    }
   ],
   "source": [
    "# for multiclass, we can inspect metrics on a per-label basis \n",
    "print(\"False positive rate by label:\")\n",
    "for i in range(len(trainingSummary.falsePositiveRateByLabel)):\n",
    "    print(\"label {}:{}\".format(i,trainingSummary.falsePositiveRateByLabel[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive rate by label:\n",
      "label 0:0.9812627291242363\n",
      "label 1:0.31756756756756754\n"
     ]
    }
   ],
   "source": [
    "print(\"True positive rate by label:\")\n",
    "for i in range(len(trainingSummary.truePositiveRateByLabel)):\n",
    "    print(\"label {}:{}\".format(i,trainingSummary.truePositiveRateByLabel[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision by label:\n",
      "label 0:0.9226350057449253\n",
      "label 1:0.6714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision by label:\")\n",
    "for i in range(len(trainingSummary.precisionByLabel)):\n",
    "    print(\"label {}:{}\".format(i,trainingSummary.precisionByLabel[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall by label:\n",
      "label 0:0.9812627291242363\n",
      "label 1:0.31756756756756754\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall by label:\")\n",
    "for i in range(len(trainingSummary.recallByLabel)):\n",
    "    print(\"label {}:{}\".format(i,trainingSummary.recallByLabel[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-measure by label:\n",
      "label 0:0.9510461902881957\n",
      "label 1:0.4311926605504587\n"
     ]
    }
   ],
   "source": [
    "print(\"F-measure by label:\")\n",
    "for i in range(len(trainingSummary.fMeasureByLabel())):\n",
    "    print(\"label {}:{}\".format(i,trainingSummary.fMeasureByLabel()[i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9098509632860778\n",
      "FPR: 0.6110206665942739\n",
      "TPR: 0.9098509632860778\n",
      "F-measure: 0.8951113866522923\n",
      "Precision: 0.895605887403362\n",
      "Rec all: 0.9098509632860778\n"
     ]
    }
   ],
   "source": [
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate \n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate \n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: {0}\\nFPR: {1}\\nTPR: {2}\\nF-measure: {3}\\nPrecision: {4}\\nRec all: {5}\".format(accuracy,falsePositiveRate,truePositiveRate,fMeasure,precision,recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
