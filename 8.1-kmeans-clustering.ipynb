{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f3c1dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark K-means example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d0063a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').\\\n",
    "                       options(header='true', \\\n",
    "                       inferschema='true').\\\n",
    "            load(\"data/iris.csv\",header=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17f91d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c0c0b",
   "metadata": {},
   "source": [
    "You can also get the Statistical resutls from the data frame (Unfortunately, it only works for numerical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaf32486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+------------------+------------------+---------+\n",
      "|summary|      sepal_length|        sepal_width|      petal_length|       petal_width|  species|\n",
      "+-------+------------------+-------------------+------------------+------------------+---------+\n",
      "|  count|               150|                150|               150|               150|      150|\n",
      "|   mean| 5.843333333333335| 3.0540000000000007|3.7586666666666693|1.1986666666666672|     null|\n",
      "| stddev|0.8280661279778637|0.43359431136217375| 1.764420419952262|0.7631607417008414|     null|\n",
      "|    min|               4.3|                2.0|               1.0|               0.1|   setosa|\n",
      "|    max|               7.9|                4.4|               6.9|               2.5|virginica|\n",
      "+-------+------------------+-------------------+------------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01216c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         features|\n",
      "+-----------------+\n",
      "|[5.1,3.5,1.4,0.2]|\n",
      "|[4.9,3.0,1.4,0.2]|\n",
      "|[4.7,3.2,1.3,0.2]|\n",
      "|[4.6,3.1,1.5,0.2]|\n",
      "|[5.0,3.6,1.4,0.2]|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 48:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "transformed=df.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label']).select(\"features\")\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d1c85",
   "metadata": {},
   "source": [
    "Deal With Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f963c0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 50:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|         features|  indexedFeatures|\n",
      "+-----------------+-----------------+\n",
      "|[5.1,3.5,1.4,0.2]|[5.1,3.5,1.4,0.2]|\n",
      "|[4.9,3.0,1.4,0.2]|[4.9,3.0,1.4,0.2]|\n",
      "|[4.7,3.2,1.3,0.2]|[4.7,3.2,1.3,0.2]|\n",
      "|[4.6,3.1,1.5,0.2]|[4.6,3.1,1.5,0.2]|\n",
      "|[5.0,3.6,1.4,0.2]|[5.0,3.6,1.4,0.2]|\n",
      "+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \\\n",
    "                               outputCol=\"indexedFeatures\",\\\n",
    "                               maxCategories=4).fit(transformed)\n",
    "\n",
    "data = featureIndexer.transform(transformed)\n",
    "data.show(5,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f8c857",
   "metadata": {},
   "source": [
    "Since clustering algorithms including k-means use distance-based measurements to determine the similarity between data points, Itâ€™s strongly recommended to standardize the data to have a mean of zero and a standard deviation of one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070f2e1",
   "metadata": {},
   "source": [
    "### StandardScaler\n",
    "\n",
    "When your dataset has uneven distribution of values, for example, some feature columns have values in millions, other feature columns have values in range between 0 and 1, you'd better to normalize all columns to have similar range of value distribuiton, Spark StandardScaler is one of such tools.\n",
    "transforms a dataset of Vector rows, normalizing each feature to have unit standard deviation and/or zero mean. It takes parameters:\n",
    "\n",
    "withStd: True by default. Scales the data to unit standard deviation.\n",
    "withMean: False by default. Centers the data with mean before scaling. It will build a dense output, so take care \n",
    "when applying to sparse input.\n",
    "\n",
    "StandardScaler is an Estimator which can be fit on a dataset to produce a StandardScalerModel; this amounts to computing summary statistics. The model can then transform a Vector column in a dataset to have unit standard deviation and/or zero mean features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60e6adaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 52:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+---------------------------------------------------------------------------------+\n",
      "|features         |indexedFeatures  |scaledFeatures                                                                   |\n",
      "+-----------------+-----------------+---------------------------------------------------------------------------------+\n",
      "|[5.1,3.5,1.4,0.2]|[5.1,3.5,1.4,0.2]|[-0.8976738791967643,1.0286112808972372,-1.3367940202882502,-1.308592819437957]  |\n",
      "|[4.9,3.0,1.4,0.2]|[4.9,3.0,1.4,0.2]|[-1.1392004834649512,-0.12454037930145648,-1.3367940202882502,-1.308592819437957]|\n",
      "|[4.7,3.2,1.3,0.2]|[4.7,3.2,1.3,0.2]|[-1.3807270877331392,0.33672028477802146,-1.393469854952817,-1.308592819437957]  |\n",
      "+-----------------+-----------------+---------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"indexedFeatures\",outputCol=\"scaledFeatures\",withStd=True, withMean=True) #check source code to confirm\n",
    "scalerModel = scaler.fit(data)\n",
    "#Normalize each feature to have unit standard deviation. \n",
    "scaledData = scalerModel.transform(data) \n",
    "scaledData.show(n=3, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ef5fb9",
   "metadata": {},
   "source": [
    "### clustering algorithm is for unsupervised learning, therefore, no training and testing splits necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92c9340b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "\n",
    "kmeans = KMeans() \\\n",
    "          .setK(3) \\\n",
    "          .setFeaturesCol(\"scaledFeatures\")\\\n",
    "          .setPredictionCol(\"cluster\")\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[kmeans])\n",
    "\n",
    "model = pipeline.fit(scaledData)\n",
    "\n",
    "cluster = model.transform(scaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa391e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+--------------------+-------+\n",
      "|         features|  indexedFeatures|      scaledFeatures|cluster|\n",
      "+-----------------+-----------------+--------------------+-------+\n",
      "|[5.1,3.5,1.4,0.2]|[5.1,3.5,1.4,0.2]|[-0.8976738791967...|      1|\n",
      "|[4.9,3.0,1.4,0.2]|[4.9,3.0,1.4,0.2]|[-1.1392004834649...|      1|\n",
      "|[4.7,3.2,1.3,0.2]|[4.7,3.2,1.3,0.2]|[-1.3807270877331...|      1|\n",
      "|[4.6,3.1,1.5,0.2]|[4.6,3.1,1.5,0.2]|[-1.5014903898672...|      1|\n",
      "|[5.0,3.6,1.4,0.2]|[5.0,3.6,1.4,0.2]|[-1.0184371813308...|      1|\n",
      "+-----------------+-----------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "753d53fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+\n",
      "|features         |cluster|\n",
      "+-----------------+-------+\n",
      "|[5.1,3.5,1.4,0.2]|1      |\n",
      "|[4.9,3.0,1.4,0.2]|1      |\n",
      "|[4.7,3.2,1.3,0.2]|1      |\n",
      "|[4.6,3.1,1.5,0.2]|1      |\n",
      "|[5.0,3.6,1.4,0.2]|1      |\n",
      "|[5.4,3.9,1.7,0.4]|1      |\n",
      "|[4.6,3.4,1.4,0.3]|1      |\n",
      "|[5.0,3.4,1.5,0.2]|1      |\n",
      "|[4.4,2.9,1.4,0.2]|1      |\n",
      "|[4.9,3.1,1.5,0.1]|1      |\n",
      "|[5.4,3.7,1.5,0.2]|1      |\n",
      "|[4.8,3.4,1.6,0.2]|1      |\n",
      "|[4.8,3.0,1.4,0.1]|1      |\n",
      "|[4.3,3.0,1.1,0.1]|1      |\n",
      "|[5.8,4.0,1.2,0.2]|1      |\n",
      "|[5.7,4.4,1.5,0.4]|1      |\n",
      "|[5.4,3.9,1.3,0.4]|1      |\n",
      "|[5.1,3.5,1.4,0.3]|1      |\n",
      "|[5.7,3.8,1.7,0.3]|1      |\n",
      "|[5.1,3.8,1.5,0.3]|1      |\n",
      "+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster.select(\"features\", \"cluster\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45f2b3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.0113575 , -0.86997056,  0.37562584,  0.31061296]),\n",
       " array([-1.01119138,  0.83949441, -1.30052149, -1.25093786]),\n",
       " array([1.16353612, 0.15326434, 0.99979607, 1.02619471])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages[0].clusterCenters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d46a0",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Silhouette_(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b030d2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.6535875501205956\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator(featuresCol='scaledFeatures', predictionCol='cluster')\n",
    "\n",
    "silhouette = evaluator.evaluate(cluster)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22491150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
