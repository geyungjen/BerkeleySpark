{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6db818c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark K-means example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0d113905",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').\\\n",
    "                       options(header='true', \\\n",
    "                       inferschema='true').\\\n",
    "            load(\"data/wine_dataset.csv\",header=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6f02f6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+-----+\n",
      "|fixed_acidity|volatile_acidity|citric_acid|residual_sugar|chlorides|free_sulfur_dioxide|total_sulfur_dioxide|density|  pH|sulphates|alcohol|quality|style|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+-----+\n",
      "|          7.4|             0.7|        0.0|           1.9|    0.076|               11.0|                34.0| 0.9978|3.51|     0.56|    9.4|      5|  red|\n",
      "|          7.8|            0.88|        0.0|           2.6|    0.098|               25.0|                67.0| 0.9968| 3.2|     0.68|    9.8|      5|  red|\n",
      "|          7.8|            0.76|       0.04|           2.3|    0.092|               15.0|                54.0|  0.997|3.26|     0.65|    9.8|      5|  red|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b5f6b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df.select(\"longitude\", \"latitude\", \"median_income\", \"ocean_proximity\")\n",
    "#df.select(\"ocean_proximity\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "64bb4442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#long=[]\n",
    "#lad=[]\n",
    "#for x in df.collect():\n",
    "#    long.append(x[0])\n",
    "#    lad.append(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f83e1cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#plt.figure(figsize = (10, 8))\n",
    "#plt.scatter(long, lad);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "03806e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[7.4,0.7,0.0,1.9,...|\n",
      "|[7.8,0.88,0.0,2.6...|\n",
      "|[7.8,0.76,0.04,2....|\n",
      "|[11.2,0.28,0.56,1...|\n",
      "|[7.4,0.7,0.0,1.9,...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13525:>                                                      (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "transformed=df.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label']).select(\"features\")\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ebbe3157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2061546d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13526:>                                                      (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|      scaledFeatures|\n",
      "+--------------------+--------------------+\n",
      "|[7.4,0.7,0.0,1.9,...|[0.14246230020601...|\n",
      "|[7.8,0.88,0.0,2.6...|[0.45100101079840...|\n",
      "|[7.8,0.76,0.04,2....|[0.45100101079840...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\",outputCol=\"scaledFeatures\",withStd=True, withMean=True)\n",
    "scalerModel = scaler.fit(transformed)\n",
    "#Normalize each feature to have unit standard deviation. \n",
    "scaledData = scalerModel.transform(transformed) \n",
    "scaledData.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "98748ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "def RunBisectKM(featuresCol='features', initK=20, dataSet=scaledData):\n",
    "    silhouette_scores=dict()\n",
    "#    evaluator = ClusteringEvaluator(featuresCol=featuresCol, metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    "    evaluator = ClusteringEvaluator(featuresCol=featuresCol, distanceMeasure='squaredEuclidean')\n",
    "\n",
    "    for K in range(2,initK):\n",
    "        KMeans_=KMeans(k=K)\n",
    "        KMeans_fit=KMeans_.fit(dataSet)\n",
    "        KMeans_transform=KMeans_fit.transform(dataSet) \n",
    "        evaluation_score=evaluator.evaluate(KMeans_transform)\n",
    "        silhouette_scores[K]=evaluation_score\n",
    "    bestK=[key for key, value in silhouette_scores.items() if value==max(silhouette_scores.values())][0]\n",
    "    return KMeans(featuresCol=featuresCol, k=bestK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a823ef68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bKM=RunBisectKM(featuresCol='features', initK=20, dataSet=scaledData)\n",
    "model=bKM.fit(scaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e3f0a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.transform(scaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ee14c524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|            features|      scaledFeatures|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|[7.4,0.7,0.0,1.9,...|[0.14246230020601...|         0|\n",
      "|[7.8,0.88,0.0,2.6...|[0.45100101079840...|         0|\n",
      "|[7.8,0.76,0.04,2....|[0.45100101079840...|         0|\n",
      "|[11.2,0.28,0.56,1...|[3.07358005083372...|         0|\n",
      "|[7.4,0.7,0.0,1.9,...|[0.14246230020601...|         0|\n",
      "+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "25a44509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         1|\n",
      "|         0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bb5ea77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14580:>                                                      (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.6891948370236831\n",
      "Cluster Centers: \n",
      "[ 7.59835334  0.40544768  0.29165352  3.13166381  0.06525729 18.59571184\n",
      " 64.81783877  0.99457328  3.25336878  0.5701578  10.76679245  5.81303602]\n",
      "[6.90813791e+00 2.87036572e-01 3.41208822e-01 7.28183975e+00\n",
      " 4.83257956e-02 4.00135399e+01 1.56531965e+02 9.94814506e-01\n",
      " 3.19086265e+00 4.99919040e-01 1.02581649e+01 5.82049135e+00]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate clustering by computing Silhouette score \n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = {}\".format(silhouette))\n",
    "# Shows the result. \n",
    "print(\"Cluster Centers: \") \n",
    "centers = model.clusterCenters()\n",
    "for i in centers:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ace9da4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ClusteringEvaluator in module pyspark.ml.evaluation:\n",
      "\n",
      "class ClusteringEvaluator(JavaEvaluator, pyspark.ml.param.shared.HasPredictionCol, pyspark.ml.param.shared.HasFeaturesCol, pyspark.ml.param.shared.HasWeightCol, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.JavaMLWritable)\n",
      " |  ClusteringEvaluator(*, predictionCol: str = 'prediction', featuresCol: str = 'features', metricName: 'ClusteringEvaluatorMetricType' = 'silhouette', distanceMeasure: str = 'squaredEuclidean', weightCol: Optional[str] = None)\n",
      " |  \n",
      " |  Evaluator for Clustering results, which expects two input\n",
      " |  columns: prediction and features. The metric computes the Silhouette\n",
      " |  measure using the squared Euclidean distance.\n",
      " |  \n",
      " |  The Silhouette is a measure for the validation of the consistency\n",
      " |  within clusters. It ranges between 1 and -1, where a value close to\n",
      " |  1 means that the points in a cluster are close to the other points\n",
      " |  in the same cluster and far from the points of the other clusters.\n",
      " |  \n",
      " |  .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from pyspark.ml.linalg import Vectors\n",
      " |  >>> featureAndPredictions = map(lambda x: (Vectors.dense(x[0]), x[1]),\n",
      " |  ...     [([0.0, 0.5], 0.0), ([0.5, 0.0], 0.0), ([10.0, 11.0], 1.0),\n",
      " |  ...     ([10.5, 11.5], 1.0), ([1.0, 1.0], 0.0), ([8.0, 6.0], 1.0)])\n",
      " |  >>> dataset = spark.createDataFrame(featureAndPredictions, [\"features\", \"prediction\"])\n",
      " |  ...\n",
      " |  >>> evaluator = ClusteringEvaluator()\n",
      " |  >>> evaluator.setPredictionCol(\"prediction\")\n",
      " |  ClusteringEvaluator...\n",
      " |  >>> evaluator.evaluate(dataset)\n",
      " |  0.9079...\n",
      " |  >>> featureAndPredictionsWithWeight = map(lambda x: (Vectors.dense(x[0]), x[1], x[2]),\n",
      " |  ...     [([0.0, 0.5], 0.0, 2.5), ([0.5, 0.0], 0.0, 2.5), ([10.0, 11.0], 1.0, 2.5),\n",
      " |  ...     ([10.5, 11.5], 1.0, 2.5), ([1.0, 1.0], 0.0, 2.5), ([8.0, 6.0], 1.0, 2.5)])\n",
      " |  >>> dataset = spark.createDataFrame(\n",
      " |  ...     featureAndPredictionsWithWeight, [\"features\", \"prediction\", \"weight\"])\n",
      " |  >>> evaluator = ClusteringEvaluator()\n",
      " |  >>> evaluator.setPredictionCol(\"prediction\")\n",
      " |  ClusteringEvaluator...\n",
      " |  >>> evaluator.setWeightCol(\"weight\")\n",
      " |  ClusteringEvaluator...\n",
      " |  >>> evaluator.evaluate(dataset)\n",
      " |  0.9079...\n",
      " |  >>> ce_path = temp_path + \"/ce\"\n",
      " |  >>> evaluator.save(ce_path)\n",
      " |  >>> evaluator2 = ClusteringEvaluator.load(ce_path)\n",
      " |  >>> str(evaluator2.getPredictionCol())\n",
      " |  'prediction'\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ClusteringEvaluator\n",
      " |      JavaEvaluator\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      Evaluator\n",
      " |      pyspark.ml.param.shared.HasPredictionCol\n",
      " |      pyspark.ml.param.shared.HasFeaturesCol\n",
      " |      pyspark.ml.param.shared.HasWeightCol\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      typing.Generic\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, predictionCol: str = 'prediction', featuresCol: str = 'features', metricName: 'ClusteringEvaluatorMetricType' = 'silhouette', distanceMeasure: str = 'squaredEuclidean', weightCol: Optional[str] = None)\n",
      " |      __init__(self, \\*, predictionCol=\"prediction\", featuresCol=\"features\",                  metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\", weightCol=None)\n",
      " |  \n",
      " |  getDistanceMeasure(self) -> 'ClusteringEvaluatorDistanceMeasureType'\n",
      " |      Gets the value of `distanceMeasure`\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  getMetricName(self) -> 'ClusteringEvaluatorMetricType'\n",
      " |      Gets the value of metricName or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  setDistanceMeasure(self, value: 'ClusteringEvaluatorDistanceMeasureType') -> 'ClusteringEvaluator'\n",
      " |      Sets the value of :py:attr:`distanceMeasure`.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  setFeaturesCol(self, value: 'str') -> 'ClusteringEvaluator'\n",
      " |      Sets the value of :py:attr:`featuresCol`.\n",
      " |  \n",
      " |  setMetricName(self, value: 'ClusteringEvaluatorMetricType') -> 'ClusteringEvaluator'\n",
      " |      Sets the value of :py:attr:`metricName`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  setParams(self, *, predictionCol: str = 'prediction', featuresCol: str = 'features', metricName: 'ClusteringEvaluatorMetricType' = 'silhouette', distanceMeasure: str = 'squaredEuclidean', weightCol: Optional[str] = None) -> 'ClusteringEvaluator'\n",
      " |      setParams(self, \\*, predictionCol=\"prediction\", featuresCol=\"features\",                   metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\", weightCol=None)\n",
      " |      Sets params for clustering evaluator.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  setPredictionCol(self, value: str) -> 'ClusteringEvaluator'\n",
      " |      Sets the value of :py:attr:`predictionCol`.\n",
      " |  \n",
      " |  setWeightCol(self, value: str) -> 'ClusteringEvaluator'\n",
      " |      Sets the value of :py:attr:`weightCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any], 'dis...\n",
      " |  \n",
      " |  __orig_bases__ = (<class 'pyspark.ml.evaluation.JavaEvaluator'>, <clas...\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  distanceMeasure = Param(parent='undefined', name='distanceMeasure'...o...\n",
      " |  \n",
      " |  metricName = Param(parent='undefined', name='metricName', doc='metric ...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from JavaEvaluator:\n",
      " |  \n",
      " |  isLargerBetter(self) -> bool\n",
      " |      Indicates whether the metric returned by :py:meth:`evaluate` should be maximized\n",
      " |      (True, default) or minimized (False).\n",
      " |      A given evaluator may support multiple metrics which may be maximized or minimized.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  clear(self, param: pyspark.ml.param.Param) -> None\n",
      " |      Clears a param from the param map if it has been explicitly set.\n",
      " |  \n",
      " |  copy(self: 'JP', extra: Optional[ForwardRef('ParamMap')] = None) -> 'JP'\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          Extra parameters to copy to the new instance\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`JavaParams`\n",
      " |          Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Evaluator:\n",
      " |  \n",
      " |  evaluate(self, dataset: pyspark.sql.dataframe.DataFrame, params: Optional[ForwardRef('ParamMap')] = None) -> float\n",
      " |      Evaluates the output with optional parameters.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset : :py:class:`pyspark.sql.DataFrame`\n",
      " |          a dataset that contains labels/observations and predictions\n",
      " |      params : dict, optional\n",
      " |          an optional param map that overrides embedded params\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          metric\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  getPredictionCol(self) -> str\n",
      " |      Gets the value of predictionCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n",
      " |  \n",
      " |  getFeaturesCol(self) -> str\n",
      " |      Gets the value of featuresCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n",
      " |  \n",
      " |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n",
      " |  \n",
      " |  getWeightCol(self) -> str\n",
      " |      Gets the value of weightCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n",
      " |  \n",
      " |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self) -> str\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra: Optional[ForwardRef('ParamMap')] = None) -> 'ParamMap'\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          extra param values\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |          merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName: str) -> bool\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path: str) -> ~RL from abc.ABCMeta\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from abc.ABCMeta\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self) -> pyspark.ml.util.JavaMLWriter\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path: str) -> None\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/21 03:39:42 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 7202858 ms exceeds timeout 120000 ms\n",
      "23/05/21 03:39:42 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "help(ClusteringEvaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff2f85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
