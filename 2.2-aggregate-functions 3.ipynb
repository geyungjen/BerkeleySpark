{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/15 12:01:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# create entry points to spark\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "sc=SparkContext()\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate functions\n",
    "Two aggregate functions:\n",
    "\n",
    "* `aggregate()`\n",
    "* `aggregateByKey()`\n",
    "\n",
    "### `aggregate(zeroValue, seqOp, combOp)`\n",
    "\n",
    "* **zeroValue** is like a data container. Its structure should match with the data structure of the returned values from the seqOp function.\n",
    "* **seqOp** is a function that takes two arguments: the first argument is the zeroValue and the second argument is an element from the RDD. The zeroValue gets updated with the returned value after every run.\n",
    "* **combOp** is a function that takes two arguments: the first argument is the final zeroValue from one partition and the other is another final zeroValue from another partition.\n",
    "\n",
    "The code below calculates the total sum of squares for **mpg** and **disp** in data set **mtcars**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: get some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(mpg=21.0, disp=160.0),\n",
       " Row(mpg=21.0, disp=160.0),\n",
       " Row(mpg=22.8, disp=108.0),\n",
       " Row(mpg=21.4, disp=258.0),\n",
       " Row(mpg=18.7, disp=360.0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcars_df = spark.read.csv('data/SparkData/mtcars.csv', inferSchema=True, header=True).select(['mpg', 'disp'])\n",
    "mtcars_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: calculate averages of mgp and disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpg mean =  20.090625000000003 ; disp mean =  230.721875\n"
     ]
    }
   ],
   "source": [
    "mpg_mean = mtcars_df.select('mpg').rdd.map(lambda x: x[0]).mean()\n",
    "disp_mean = mtcars_df.select('disp').rdd.map(lambda x: x[0]).mean()\n",
    "print('mpg mean = ', mpg_mean, '; ' \n",
    "      'disp mean = ', disp_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: build **zeroValue, seqOp** and **combOp**\n",
    "\n",
    "We are calculating two TSS. We create a tuple to store two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroValue = (0, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **z** below refers to `zeroValue`. Its values get updated after every run. The **x** refers to an element in an RDD partition. In this case, both **z** and **x** have two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqOp = lambda z, x: (z[0] + (x[0] - mpg_mean)**2, z[1] + (x[1] - disp_mean)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `combOp` function simply aggrate all `zeroValues` into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combOp = lambda px, py: ( px[0] + py[0], px[1] + py[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement `aggregate()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1126.0471874999998, 476184.7946875)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcars_df.rdd.aggregate(zeroValue, seqOp, combOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `aggregateByKey(zeroValue, seqOp, combOp)`\n",
    "\n",
    "This function does similar things as `aggregate()`. The `aggregate()` aggregate all results to the very end, but aggregateByKey() merge results by key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first calculate word count, i.e., calculate/aggregate the counts based on the same key, i.e., word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=['hello', 'world', 'good', 'hello']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an RDD, each element of the RDD to be a tuple, first element is key, which is word, second element is 1 to be counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz=sc.parallelize(x).map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define initial value, seqOp and combOp, since this is addition, initial value is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_value = 0\n",
    "seqOp = (lambda x, y: x + y) #add up all the 1 based on the same key\n",
    "combOp = (lambda x, y: x + y) #Merge all the word count across partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 1), ('world', 1), ('good', 1), ('hello', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 1), ('hello', 2), ('world', 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz.aggregateByKey(zero_value,seqOp,combOp).collect()  #Now you have word count, counts based on words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('data/SparkData/mtcars.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/15 12:15:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n",
      " Schema: _c0, mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/user/BerkeleySpark/data/SparkData/mtcars.csv\n",
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|              _c0| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n",
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|        Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|\n",
      "|    Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|\n",
      "|       Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n",
      "|   Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|\n",
      "|Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|\n",
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SparkSQL dataframe that contains on 3 columns, cyl (cylinder, mpg and disp (displacement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df=df.select(['cyl', 'mpg', 'disp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "|cyl| mpg| disp|\n",
      "+---+----+-----+\n",
      "|  6|21.0|160.0|\n",
      "|  6|21.0|160.0|\n",
      "|  4|22.8|108.0|\n",
      "|  6|21.4|258.0|\n",
      "|  8|18.7|360.0|\n",
      "+---+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "car_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create another dataframe that contains cylinder, mean mpg and mean displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df.createOrReplaceTempView('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_mean_df=spark.sql(\"select cyl as cylinder, avg(mpg) as mean_mpg, avg(disp) as mean_disp from car group by 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+------------------+\n",
      "|cylinder|          mean_mpg|         mean_disp|\n",
      "+--------+------------------+------------------+\n",
      "|       6| 19.74285714285714|183.31428571428572|\n",
      "|       4|26.663636363636364|105.13636363636364|\n",
      "|       8|15.100000000000003|353.09999999999997|\n",
      "+--------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "car_mean_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+\n",
      "|cylinder| mpg| disp|\n",
      "+--------+----+-----+\n",
      "|       6|21.0|160.0|\n",
      "|       6|21.0|160.0|\n",
      "|       4|22.8|108.0|\n",
      "|       6|21.4|258.0|\n",
      "|       8|18.7|360.0|\n",
      "+--------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+------------------+------------------+\n",
      "|cylinder|          mean_mpg|         mean_disp|\n",
      "+--------+------------------+------------------+\n",
      "|       6| 19.74285714285714|183.31428571428572|\n",
      "|       4|26.663636363636364|105.13636363636364|\n",
      "|       8|15.100000000000003|353.09999999999997|\n",
      "+--------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "car_df=car_df.withColumnRenamed(\"cyl\",\"cylinder\") #rename the column cyl of car_df to cylinder\n",
    "car_df.show(5)\n",
    "car_mean_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new dataframe that combines mpg, average mpg, displacement, average displacement along with cylinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+------------------+-----+------------------+\n",
      "|cylinder| mpg|          mean_mpg| disp|         mean_disp|\n",
      "+--------+----+------------------+-----+------------------+\n",
      "|       6|21.0| 19.74285714285714|160.0|183.31428571428572|\n",
      "|       6|21.0| 19.74285714285714|160.0|183.31428571428572|\n",
      "|       4|22.8|26.663636363636364|108.0|105.13636363636364|\n",
      "|       6|21.4| 19.74285714285714|258.0|183.31428571428572|\n",
      "|       8|18.7|15.100000000000003|360.0|353.09999999999997|\n",
      "+--------+----+------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "car_df.createOrReplaceTempView(\"car_df\")\n",
    "car_mean_df.createOrReplaceTempView(\"car_mean_df\")\n",
    "car_mean_combined_df=spark.sql(\"select a.cylinder, a.mpg, b.mean_mpg, a.disp, b.mean_disp from car_df a inner join \\\n",
    "           car_mean_df b on a.cylinder = b.cylinder\")\n",
    "car_mean_combined_df.show(5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now create a RDD, each element of RDD is a list, 1st element of the list is always Key, Cylinder, 2nd element of the list is value, a tuple \n",
    "\n",
    "[cylinder, (mpg, mean_mpg, disp, mean_disp) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_mean_combined_rdd=car_mean_combined_df.rdd.map(lambda x: [x[0], (x[1], x[2], x[3], x[4])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, (21.0, 19.74285714285714, 160.0, 183.31428571428572)],\n",
       " [6, (21.0, 19.74285714285714, 160.0, 183.31428571428572)],\n",
       " [4, (22.8, 26.663636363636364, 108.0, 105.13636363636364)],\n",
       " [6, (21.4, 19.74285714285714, 258.0, 183.31428571428572)]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_mean_combined_rdd.take(4)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new RDD that contains key and value:\n",
    "\n",
    "[cynlinder, (mean suare error of mpg, mean square error of disp)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, (1.5804081632653129, 543.5559183673471)],\n",
       " [6, (1.5804081632653129, 543.5559183673471)],\n",
       " [4, (14.92768595041322, 8.200413223140474)],\n",
       " [6, (2.746122448979596, 5577.955918367346)],\n",
       " [8, (12.959999999999972, 47.61000000000047)]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_mean_square_rdd=car_mean_combined_rdd.map(lambda x: [x[0], ((x[1][0]-x[1][1])**2, (x[1][2]-x[1][3])**2)])\n",
    "car_mean_square_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the squqre mean error between sum((mpg-mean_mpg)^2) and sum((disp-mean_disp)^) based on same key\n",
    "which is cylinder, and use aggregateByKey API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_value = (0, 0) #zero_value[0] is for mpg, zero_value[1] is for disp\n",
    "seqOp = (lambda x, y: (x[0]+y[0], x[1]+y[1])) #add up all mean square error of mpg, add up all mean square error disp\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1])) #merge all mean square errors across partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, (12.677142857142847, 10364.628571428573)),\n",
       " (4, (203.38545454545448, 7220.825454545454)),\n",
       " (8, (85.19999999999999, 59708.38))]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_mean_square_rdd.aggregateByKey(zero_value, seqOp, combOp).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
