{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/03 00:11:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master = 'local')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "          .appName(\"Python Spark SQL basic example\") \\\n",
    "          .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "          .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+---+\n",
      "|age|education|wantsMore|  y|\n",
      "+---+---------+---------+---+\n",
      "|<25|      low|      yes|  0|\n",
      "|<25|      low|      yes|  0|\n",
      "|<25|      low|      yes|  0|\n",
      "|<25|      low|      yes|  0|\n",
      "|<25|      low|      yes|  0|\n",
      "+---+---------+---------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cuse = spark.read.csv('data/cuse_binary.csv', header=True, inferSchema=True)\n",
    "cuse.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {Row(age='<25'): 397,\n",
       "             Row(age='25-29'): 404,\n",
       "             Row(age='30-39'): 612,\n",
       "             Row(age='40-49'): 194})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuse.columns[0:3]\n",
    "# cuse.select('age').distinct().show()\n",
    "cuse.select('age').rdd.countByValue()\n",
    "# cuse.select('education').rdd.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  y|\n",
      "+---+\n",
      "|  1|\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cuse.select(\"y\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|  age|indexed_age|\n",
      "+-----+-----------+\n",
      "|30-39|        0.0|\n",
      "|  <25|        2.0|\n",
      "|25-29|        1.0|\n",
      "|40-49|        3.0|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# string index each categorical string columns\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=\"indexed_\"+column) for column in ('age', 'education', 'wantsMore')]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "indexed_cuse = pipeline.fit(cuse).transform(cuse)\n",
    "indexed_cuse.select('age', 'indexed_age').distinct().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+----------------------+---+\n",
      "|onehotencode_age|onehotencode_education|onehotencode_wantsMore|  y|\n",
      "+----------------+----------------------+----------------------+---+\n",
      "|   (3,[1],[1.0])|             (1,[],[])|         (1,[0],[1.0])|  0|\n",
      "|   (3,[2],[1.0])|         (1,[0],[1.0])|             (1,[],[])|  1|\n",
      "|   (3,[0],[1.0])|         (1,[0],[1.0])|         (1,[0],[1.0])|  0|\n",
      "|       (3,[],[])|         (1,[0],[1.0])|         (1,[0],[1.0])|  1|\n",
      "|   (3,[2],[1.0])|             (1,[],[])|         (1,[0],[1.0])|  0|\n",
      "+----------------+----------------------+----------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# onehotencode each indexed categorical columns\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "columns = indexed_cuse.columns[0:3]\n",
    "onehoteencoders = [OneHotEncoder(inputCol=\"indexed_\"+column, outputCol=\"onehotencode_\"+column) for column in columns]\n",
    "pipeline = Pipeline(stages=onehoteencoders)\n",
    "onehotencode_columns = ['onehotencode_age', 'onehotencode_education', 'onehotencode_wantsMore', 'y']\n",
    "onehotencode_cuse = pipeline.fit(indexed_cuse).transform(indexed_cuse).select(onehotencode_columns)\n",
    "onehotencode_cuse.distinct().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+----------------------+-----+-------------------+\n",
      "|onehotencode_age|onehotencode_education|onehotencode_wantsMore|label|           features|\n",
      "+----------------+----------------------+----------------------+-----+-------------------+\n",
      "|   (3,[2],[1.0])|             (1,[],[])|         (1,[0],[1.0])|    0|(5,[2,4],[1.0,1.0])|\n",
      "|   (3,[2],[1.0])|             (1,[],[])|         (1,[0],[1.0])|    0|(5,[2,4],[1.0,1.0])|\n",
      "|   (3,[2],[1.0])|             (1,[],[])|         (1,[0],[1.0])|    0|(5,[2,4],[1.0,1.0])|\n",
      "|   (3,[2],[1.0])|             (1,[],[])|         (1,[0],[1.0])|    0|(5,[2,4],[1.0,1.0])|\n",
      "|   (3,[2],[1.0])|             (1,[],[])|         (1,[0],[1.0])|    0|(5,[2,4],[1.0,1.0])|\n",
      "+----------------+----------------------+----------------------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assemble all feature columns into on single vector column\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=['onehotencode_age', 'onehotencode_education', 'onehotencode_wantsMore'], outputCol='features')\n",
    "cuse_df_2 = assembler.transform(onehotencode_cuse).withColumnRenamed('y', 'label')\n",
    "cuse_df_2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+----------------------+-----+---------+\n",
      "|onehotencode_age|onehotencode_education|onehotencode_wantsMore|label| features|\n",
      "+----------------+----------------------+----------------------+-----+---------+\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|\n",
      "+----------------+----------------------+----------------------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split data into training and test datasets\n",
    "training, test = cuse_df_2.randomSplit([0.8, 0.2], seed=1234)\n",
    "training.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ======= build cross validation model ===========\n",
    "\n",
    "# estimator\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "glm = GeneralizedLinearRegression(featuresCol='features', labelCol='label', family='binomial')\n",
    "\n",
    "# parameter grid\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(glm.regParam, [0, 0.5, 1, 2, 4]).\\\n",
    "    build()\n",
    "    \n",
    "# evaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction')\n",
    "\n",
    "# build cross-validation model\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=glm, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/03 00:14:32 WARN Instrumentation: [eae74233] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:32 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/05/03 00:14:32 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/05/03 00:14:32 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "23/05/03 00:14:33 WARN Instrumentation: [eae74233] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:33 WARN Instrumentation: [eae74233] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:33 WARN Instrumentation: [eae74233] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:33 WARN Instrumentation: [eae74233] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:33 WARN Instrumentation: [eae74233] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:35 WARN Instrumentation: [cdfedc94] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:35 WARN Instrumentation: [cdfedc94] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:35 WARN Instrumentation: [cdfedc94] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:35 WARN Instrumentation: [cdfedc94] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:35 WARN Instrumentation: [cdfedc94] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:35 WARN Instrumentation: [cdfedc94] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:37 WARN Instrumentation: [19fac777] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:37 WARN Instrumentation: [19fac777] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:37 WARN Instrumentation: [19fac777] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:37 WARN Instrumentation: [19fac777] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:37 WARN Instrumentation: [19fac777] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:37 WARN Instrumentation: [19fac777] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:39 WARN Instrumentation: [189b4fad] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:39 WARN Instrumentation: [189b4fad] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:39 WARN Instrumentation: [189b4fad] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:39 WARN Instrumentation: [189b4fad] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:39 WARN Instrumentation: [189b4fad] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:39 WARN Instrumentation: [189b4fad] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:40 WARN Instrumentation: [f00995bc] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:40 WARN Instrumentation: [f00995bc] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:40 WARN Instrumentation: [f00995bc] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:41 WARN Instrumentation: [f00995bc] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:41 WARN Instrumentation: [f00995bc] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/03 00:14:41 WARN Instrumentation: [f00995bc] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "# cv_model = cv.fit(training)\n",
    "cv_model = cv.fit(cuse_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+----------------------+-----+---------+------------------+\n",
      "|onehotencode_age|onehotencode_education|onehotencode_wantsMore|label| features|        prediction|\n",
      "+----------------+----------------------+----------------------+-----+---------+------------------+\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151293|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151293|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151293|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151293|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151293|\n",
      "+----------------+----------------------+----------------------+-----+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+----------------------+----------------------+-----+---------+------------------+\n",
      "|onehotencode_age|onehotencode_education|onehotencode_wantsMore|label|features |prediction        |\n",
      "+----------------+----------------------+----------------------+-----+---------+------------------+\n",
      "|(3,[],[])       |(1,[],[])             |(1,[],[])             |0    |(5,[],[])|0.5140024065151293|\n",
      "|(3,[],[])       |(1,[],[])             |(1,[],[])             |0    |(5,[],[])|0.5140024065151293|\n",
      "|(3,[],[])       |(1,[],[])             |(1,[],[])             |0    |(5,[],[])|0.5140024065151293|\n",
      "|(3,[],[])       |(1,[],[])             |(1,[],[])             |0    |(5,[],[])|0.5140024065151293|\n",
      "|(3,[],[])       |(1,[],[])             |(1,[],[])             |0    |(5,[],[])|0.5140024065151293|\n",
      "+----------------+----------------------+----------------------+-----+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "pred_training_cv = cv_model.transform(training)\n",
    "pred_test_cv = cv_model.transform(test)\n",
    "\n",
    "pred_training_cv.show(5)\n",
    "pred_test_cv.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.2806, -0.7999, -1.1892, 0.325, -0.833])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.bestModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056024275169240606"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.bestModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6716478245974649"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(pred_training_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6830864197530864"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(pred_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
