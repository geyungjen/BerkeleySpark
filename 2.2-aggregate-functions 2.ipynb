{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create entry points to spark\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "sc=SparkContext()\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate functions\n",
    "Two aggregate functions:\n",
    "\n",
    "* `aggregate()`\n",
    "* `aggregateByKey()`\n",
    "\n",
    "### `aggregate(zeroValue, seqOp, combOp)`\n",
    "\n",
    "* **zeroValue** is like a data container. Its structure should match with the data structure of the returned values from the seqOp function.\n",
    "* **seqOp** is a function that takes two arguments: the first argument is the zeroValue and the second argument is an element from the RDD. The zeroValue gets updated with the returned value after every run.\n",
    "* **combOp** is a function that takes two arguments: the first argument is the final zeroValue from one partition and the other is another final zeroValue from another partition.\n",
    "\n",
    "The code below calculates the total sum of squares for **mpg** and **disp** in data set **mtcars**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: get some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(mpg=21.0, disp=160.0),\n",
       " Row(mpg=21.0, disp=160.0),\n",
       " Row(mpg=22.8, disp=108.0),\n",
       " Row(mpg=21.4, disp=258.0),\n",
       " Row(mpg=18.7, disp=360.0)]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcars_df = spark.read.csv('data/SparkData/mtcars.csv', inferSchema=True, header=True).select(['mpg', 'disp'])\n",
    "mtcars_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: calculate averages of mgp and disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpg mean =  20.090625000000003 ; disp mean =  230.721875\n"
     ]
    }
   ],
   "source": [
    "mpg_mean = mtcars_df.select('mpg').rdd.map(lambda x: x[0]).mean()\n",
    "disp_mean = mtcars_df.select('disp').rdd.map(lambda x: x[0]).mean()\n",
    "print('mpg mean = ', mpg_mean, '; ' \n",
    "      'disp mean = ', disp_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: build **zeroValue, seqOp** and **combOp**\n",
    "\n",
    "We are calculating two TSS. We create a tuple to store two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroValue = (0, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **z** below refers to `zeroValue`. Its values get updated after every run. The **x** refers to an element in an RDD partition. In this case, both **z** and **x** have two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqOp = lambda z, x: (z[0] + (x[0] - mpg_mean)**2, z[1] + (x[1] - disp_mean)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `combOp` function simply aggrate all `zeroValues` into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "combOp = lambda px, py: ( px[0] + py[0], px[1] + py[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement `aggregate()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1126.0471874999998, 476184.7946875)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcars_df.rdd.aggregate(zeroValue, seqOp, combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `aggregateByKey(zeroValue, seqOp, combOp)`\n",
    "\n",
    "This function does similar things as `aggregate()`. The `aggregate()` aggregate all results to the very end, but aggregateByKey() merge results by key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=['hello', 'world', 'good', 'hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_value = 0\n",
    "seqOp = (lambda x, y: x + y)\n",
    "combOp = (lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz=sc.parallelize(x).map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 1), ('world', 1), ('good', 1), ('hello', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 1), ('hello', 2), ('world', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz.aggregateByKey(zero_value,seqOp,combOp).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal_length,sepal_width,petal_length,petal_width,species',\n",
       " '5.1,3.5,1.4,0.2,setosa']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_rdd = sc.textFile('data/SparkData/iris.csv', use_unicode=True)\n",
    "iris_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data to a tuple RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('setosa', [5.1, 3.5, 1.4, 0.2]),\n",
       " ('setosa', [4.9, 3.0, 1.4, 0.2]),\n",
       " ('setosa', [4.7, 3.2, 1.3, 0.2]),\n",
       " ('setosa', [4.6, 3.1, 1.5, 0.2]),\n",
       " ('setosa', [5.0, 3.6, 1.4, 0.2])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_rdd_2 = iris_rdd.map(lambda x: x.split(',')).\\\n",
    "    filter(lambda x: x[0] != 'sepal_length').\\\n",
    "    map(lambda x: (x[-1], [*map(float, x[:-1])]))\n",
    "iris_rdd_2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define initial values, seqOp and combOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_value = (0, 0, 0, 0)\n",
    "seqOp = (lambda x, y: (x[0] + (y[0])**2, x[1] + (y[1])**2, x[2] + (y[2])**2, x[3] + (y[3])**2))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1], x[2]+y[2], x[3] + y[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement `aggregateByKey()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('setosa',\n",
       "  (1259.0899999999997,\n",
       "   591.2500000000002,\n",
       "   108.63999999999997,\n",
       "   3.5400000000000005)),\n",
       " ('versicolor', (1774.8600000000001, 388.47, 918.2, 89.83)),\n",
       " ('virginica', (2189.9000000000005, 447.33, 1556.1599999999994, 208.93))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_rdd_2.aggregateByKey(zero_value, seqOp, combOp).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = sc.parallelize([\n",
    "                    Vectors.dense(0.0, 1.0, 2.0),\n",
    "                    Vectors.dense(3.0, 4.0, 5.0),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = RowMatrix(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 58:==================================================>       (7 + 1) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "svd = mat.computeSVD(2, computeU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = svd.U       # The U factor is a RowMatrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = svd.s       # The singular values are stored in a local dense vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = svd.V       # The V factor is a local dense matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.numRows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.numCols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.numRows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.27472112789737796, -0.9615239476408239],\n",
       " [-0.9615239476408228, 0.27472112789737846]]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uMatrix=[]\n",
    "for i in U.rows.collect():\n",
    "    uMatrix.append([i[0],i[1]])\n",
    "uMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-2.60154843e-01, -1.90043939e-01,  8.80774787e-01,\n",
       "          3.47042554e-01, -4.42378223e-09, -9.31322575e-09,\n",
       "         -2.03726813e-09],\n",
       "        [-4.87250158e-01, -1.62282993e-01,  1.53686688e-01,\n",
       "         -8.44175287e-01, -9.31322575e-10,  1.49011612e-08,\n",
       "         -2.32830644e-10],\n",
       "        [-7.58264730e-01, -2.56908256e-01, -4.39763758e-01,\n",
       "          4.06989662e-01, -3.16649675e-08,  1.49011612e-08,\n",
       "          7.79982656e-09],\n",
       "        [-3.46325484e-01,  9.33566109e-01,  8.49942012e-02,\n",
       "          3.59021663e-02, -1.28056854e-09, -2.79396772e-09,\n",
       "         -1.26892701e-08]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "u=np.matrix(uMatrix)\n",
    "uT=u.transpose()\n",
    "u\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-2.60154843e-01, -4.87250158e-01, -7.58264730e-01,\n",
       "         -3.46325484e-01],\n",
       "        [-1.90043939e-01, -1.62282993e-01, -2.56908256e-01,\n",
       "          9.33566109e-01],\n",
       "        [ 8.80774787e-01,  1.53686688e-01, -4.39763758e-01,\n",
       "          8.49942012e-02],\n",
       "        [ 3.47042554e-01, -8.44175287e-01,  4.06989662e-01,\n",
       "          3.59021663e-02],\n",
       "        [-4.42378223e-09, -9.31322575e-10, -3.16649675e-08,\n",
       "         -1.28056854e-09],\n",
       "        [-9.31322575e-09,  1.49011612e-08,  1.49011612e-08,\n",
       "         -2.79396772e-09],\n",
       "        [-2.03726813e-09, -2.32830644e-10,  7.79982656e-09,\n",
       "         -1.26892701e-08]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.10379724,  0.15760139,  0.2460901 , -0.08732033],\n",
       "        [ 0.15760139,  0.26374849,  0.41115645,  0.01724524],\n",
       "        [ 0.2460901 ,  0.41115645,  0.64096725,  0.02276556],\n",
       "        [-0.08732033,  0.01724524,  0.02276556,  0.99148702]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(u,uT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.34846923, 1.        ])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method computeSVD in module pyspark.mllib.linalg.distributed:\n",
      "\n",
      "computeSVD(k: int, computeU: bool = False, rCond: float = 1e-09) -> 'SingularValueDecomposition[RowMatrix, Matrix]' method of pyspark.mllib.linalg.distributed.RowMatrix instance\n",
      "    Computes the singular value decomposition of the RowMatrix.\n",
      "    \n",
      "    The given row matrix A of dimension (m X n) is decomposed into\n",
      "    U * s * V'T where\n",
      "    \n",
      "    - U: (m X k) (left singular vectors) is a RowMatrix whose\n",
      "      columns are the eigenvectors of (A X A')\n",
      "    - s: DenseVector consisting of square root of the eigenvalues\n",
      "      (singular values) in descending order.\n",
      "    - v: (n X k) (right singular vectors) is a Matrix whose columns\n",
      "      are the eigenvectors of (A' X A)\n",
      "    \n",
      "    For more specific details on implementation, please refer\n",
      "    the Scala documentation.\n",
      "    \n",
      "    .. versionadded:: 2.2.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    k : int\n",
      "        Number of leading singular values to keep (`0 < k <= n`).\n",
      "        It might return less than k if there are numerically zero singular values\n",
      "        or there are not enough Ritz values converged before the maximum number of\n",
      "        Arnoldi update iterations is reached (in case that matrix A is ill-conditioned).\n",
      "    computeU : bool, optional\n",
      "        Whether or not to compute U. If set to be\n",
      "        True, then U is computed by A * V * s^-1\n",
      "    rCond : float, optional\n",
      "        Reciprocal condition number. All singular values\n",
      "        smaller than rCond * s[0] are treated as zero\n",
      "        where s[0] is the largest singular value.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :py:class:`SingularValueDecomposition`\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> rows = sc.parallelize([[3, 1, 1], [-1, 3, 1]])\n",
      "    >>> rm = RowMatrix(rows)\n",
      "    \n",
      "    >>> svd_model = rm.computeSVD(2, True)\n",
      "    >>> svd_model.U.rows.collect()\n",
      "    [DenseVector([-0.7071, 0.7071]), DenseVector([-0.7071, -0.7071])]\n",
      "    >>> svd_model.s\n",
      "    DenseVector([3.4641, 3.1623])\n",
      "    >>> svd_model.V\n",
      "    DenseMatrix(3, 2, [-0.4082, -0.8165, -0.4082, 0.8944, -0.4472, 0.0], 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(mat.computeSVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.39254051, -0.56077215, -0.7290038 ,  0.82416338,  0.13736056,\n",
       "       -0.54944226])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.numRows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.numCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('data/SparkData/mtcars.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/15 10:37:06 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n",
      " Schema: _c0, mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/user/BerkeleySpark/data/SparkData/mtcars.csv\n",
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|              _c0| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n",
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|        Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|\n",
      "|    Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|\n",
      "|       Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n",
      "|   Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|\n",
      "|Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|\n",
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df=df.select(['cyl', 'mpg', 'disp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "|cyl| mpg| disp|\n",
      "+---+----+-----+\n",
      "|  6|21.0|160.0|\n",
      "|  6|21.0|160.0|\n",
      "|  4|22.8|108.0|\n",
      "|  6|21.4|258.0|\n",
      "|  8|18.7|360.0|\n",
      "+---+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "car_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df.createOrReplaceTempView('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_mean_df=spark.sql(\"select cyl as cylinder, avg(mpg) as mean_mpg, avg(disp) as mean_disp from car group by 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+------------------+\n",
      "|cylinder|          mean_mpg|         mean_disp|\n",
      "+--------+------------------+------------------+\n",
      "|       6| 19.74285714285714|183.31428571428572|\n",
      "|       4|26.663636363636364|105.13636363636364|\n",
      "|       8|15.100000000000003|353.09999999999997|\n",
      "+--------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "car_mean_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+\n",
      "|cylinder| mpg| disp|\n",
      "+--------+----+-----+\n",
      "|       6|21.0|160.0|\n",
      "|       6|21.0|160.0|\n",
      "|       4|22.8|108.0|\n",
      "|       6|21.4|258.0|\n",
      "|       8|18.7|360.0|\n",
      "+--------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+------------------+------------------+\n",
      "|cylinder|          mean_mpg|         mean_disp|\n",
      "+--------+------------------+------------------+\n",
      "|       6| 19.74285714285714|183.31428571428572|\n",
      "|       4|26.663636363636364|105.13636363636364|\n",
      "|       8|15.100000000000003|353.09999999999997|\n",
      "+--------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "car_df=car_df.withColumnRenamed(\"cyl\",\"cylinder\")\n",
    "car_df.show(5)\n",
    "car_mean_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+------------------+-----+------------------+\n",
      "|cylinder| mpg|          mean_mpg| disp|         mean_disp|\n",
      "+--------+----+------------------+-----+------------------+\n",
      "|       6|21.0| 19.74285714285714|160.0|183.31428571428572|\n",
      "|       6|21.0| 19.74285714285714|160.0|183.31428571428572|\n",
      "|       4|22.8|26.663636363636364|108.0|105.13636363636364|\n",
      "|       6|21.4| 19.74285714285714|258.0|183.31428571428572|\n",
      "|       8|18.7|15.100000000000003|360.0|353.09999999999997|\n",
      "+--------+----+------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "car_df.createOrReplaceTempView(\"car_df\")\n",
    "car_mean_df.createOrReplaceTempView(\"car_mean_df\")\n",
    "car_mean_combined_df=spark.sql(\"select a.cylinder, a.mpg, b.mean_mpg, a.disp, b.mean_disp from car_df a inner join \\\n",
    "           car_mean_df b on a.cylinder = b.cylinder\")\n",
    "car_mean_combined_df.show(5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_mean_combined_rdd=car_mean_combined_df.rdd.map(lambda x: (x[0], (x[1], x[2], x[3], x[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, (21.0, 19.74285714285714, 160.0, 183.31428571428572)),\n",
       " (6, (21.0, 19.74285714285714, 160.0, 183.31428571428572)),\n",
       " (4, (22.8, 26.663636363636364, 108.0, 105.13636363636364)),\n",
       " (6, (21.4, 19.74285714285714, 258.0, 183.31428571428572))]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_mean_combined_rdd.take(4)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, (1.5804081632653129, 543.5559183673471)],\n",
       " [6, (1.5804081632653129, 543.5559183673471)],\n",
       " [4, (14.92768595041322, 8.200413223140474)],\n",
       " [6, (2.746122448979596, 5577.955918367346)],\n",
       " [8, (12.959999999999972, 47.61000000000047)]]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_mean_square_rdd=car_mean_combined_rdd.map(lambda x: [x[0], ((x[1][0]-x[1][1])**2, (x[1][2]-x[1][3])**2)])\n",
    "car_mean_square_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the squqre mean error between sum((mpg-mean_mpg)^2) and sum((disp-mean_disp)^) based on same key\n",
    "which is model, and use aggregateByKey API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_value = (0, 0) #zero_value[0] is for mpg, zero_value[1] is for disp\n",
    "seqOp = (lambda x, y: (x[0]+y[0], x[1]+y[1])) \n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, (12.677142857142847, 10364.628571428573)),\n",
       " (4, (203.38545454545448, 7220.825454545454)),\n",
       " (8, (85.19999999999999, 59708.38))]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_mean_square_rdd.aggregateByKey(zero_value, seqOp, combOp).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
